{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210295d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CASMO: Confident Adaptive Selective Momentum Optimizer\n",
    "\n",
    "A production-ready PyTorch optimizer that extends Adam with confidence-based learning rate scaling.\n",
    "\n",
    "Core Innovation: AGAR (Adaptive Gradient Alignment Ratio)\n",
    "    AGAR = ||E[g]||² / (||E[g]||² + Var[g])\n",
    "    \n",
    "    Measures signal (consistent gradient direction) vs noise (random fluctuations).\n",
    "    Naturally ranges from 0 (pure noise) to 1 (pure signal) for interpretable confidence metrics.\n",
    "\n",
    "Performance:\n",
    "    - Faster than AdamW on large models (-2% overhead with per-group mode)\n",
    "    - Configurable granularity for speed/precision tradeoff\n",
    "    - Pre-allocated buffers eliminate allocation overhead\n",
    "\n",
    "Usage Example:\n",
    "    >>> from casmo import CASMO\n",
    "    >>> optimizer = CASMO(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    >>> for epoch in range(num_epochs):\n",
    "    ...     for batch in dataloader:\n",
    "    ...         loss = model(batch)\n",
    "    ...         loss.backward()\n",
    "    ...         optimizer.step()\n",
    "    ...         optimizer.zero_grad()\n",
    "\n",
    "Reference: \n",
    "    Kingma & Ba (2015). \"Adam: A Method for Stochastic Optimization\"\n",
    "    https://arxiv.org/abs/1412.6980\n",
    "\"\"\"\n",
    "\n",
    "from typing import Tuple, Optional, Callable, Dict, Any\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import logging\n",
    "\n",
    "\n",
    "class DDEAdapter:\n",
    "    \"\"\"\n",
    "    Drift-Detecting EMA adapter for tau threshold adjustment.\n",
    "    \n",
    "    Tracks AGAR variance to adaptively adjust tau while preventing\n",
    "    runaway adaptation to noise or memorization signals.\n",
    "    O(1) memory and compute per step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # EMA update rates\n",
    "    EMA_MEAN_RATE = 0.001\n",
    "    EMA_VAR_DECAY = 0.99\n",
    "    EMA_VAR_RATE = 0.01\n",
    "    \n",
    "    # Adaptive gain bounds\n",
    "    MIN_GAIN = 0.001\n",
    "    MAX_GAIN = 0.01\n",
    "    GAIN_SCALE = 0.1\n",
    "    \n",
    "    # Memorization detection threshold\n",
    "    MEMORIZATION_FACTOR = 1.2\n",
    "    \n",
    "    def __init__(self, tau_init: float, tau_clip_range: Tuple[float, float], \n",
    "                 dead_zone_factor: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the DDE adapter.\n",
    "        \n",
    "        Args:\n",
    "            tau_init: Initial tau value\n",
    "            tau_clip_range: (min, max) bounds for tau\n",
    "            dead_zone_factor: Ignore deviations smaller than this fraction of tau.\n",
    "                Prevents chasing noise. Default: 0.2 (20%)\n",
    "        \"\"\"\n",
    "        self.tau = tau_init\n",
    "        self.tau_calibrated: Optional[float] = None\n",
    "        self.clip_range = tau_clip_range\n",
    "        self.dead_zone = dead_zone_factor\n",
    "        \n",
    "        # EMA state for variance tracking\n",
    "        self.mean_agar = tau_init\n",
    "        self.ema_var = 0.01\n",
    "    \n",
    "    def update(self, agar_value: float) -> float:\n",
    "        \"\"\"\n",
    "        Update tau threshold using variance-adaptive gain and dead zone filtering.\n",
    "        \n",
    "        Args:\n",
    "            agar_value: Current AGAR measurement\n",
    "            \n",
    "        Returns:\n",
    "            Updated tau value (clipped to valid range)\n",
    "        \"\"\"\n",
    "        # Update EMA mean\n",
    "        diff = agar_value - self.mean_agar\n",
    "        self.mean_agar += self.EMA_MEAN_RATE * diff\n",
    "        \n",
    "        # Update EMA variance: Var[X] = E[(X - μ)²]\n",
    "        self.ema_var = self.EMA_VAR_DECAY * self.ema_var + self.EMA_VAR_RATE * (diff ** 2)\n",
    "        \n",
    "        # Relative variance (scale-invariant)\n",
    "        rel_var = self.ema_var / (self.mean_agar + 1e-8)\n",
    "        \n",
    "        # Prevent tau from chasing memorization signals\n",
    "        if self.tau_calibrated is not None and agar_value > self.MEMORIZATION_FACTOR * self.tau_calibrated:\n",
    "            # AGAR suspiciously high - likely overfitting, freeze tau\n",
    "            return self.tau\n",
    "        \n",
    "        # Dead zone: only adapt if deviation exceeds threshold\n",
    "        dead_zone_reference = self.tau_calibrated if self.tau_calibrated is not None else self.tau\n",
    "        deviation = abs(agar_value - self.tau)\n",
    "        if deviation > self.dead_zone * dead_zone_reference:\n",
    "            # Variance-adaptive gain: higher variance → faster adaptation\n",
    "            alpha = self.MIN_GAIN + min(rel_var * self.GAIN_SCALE, self.MAX_GAIN - self.MIN_GAIN)\n",
    "            new_tau = (1 - alpha) * self.tau + alpha * agar_value\n",
    "            \n",
    "            # Never decrease tau below calibrated baseline\n",
    "            if self.tau_calibrated is not None:\n",
    "                new_tau = max(new_tau, self.tau_calibrated)\n",
    "            \n",
    "            self.tau = new_tau\n",
    "        \n",
    "        return float(np.clip(self.tau, self.clip_range[0], self.clip_range[1]))\n",
    "\n",
    "\n",
    "class CASMO(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Confident Adaptive Selective Momentum Optimizer.\n",
    "    \n",
    "    Extends Adam with confidence-based learning rate scaling using AGAR metrics.\n",
    "    Automatically adapts to gradient signal-to-noise ratio for improved convergence.\n",
    "    \n",
    "    Uses universal sigmoid-based confidence mapping that adapts to any noise distribution:\n",
    "    - Clean data: High confidence baseline\n",
    "    - Pervasive noise: Adaptive scaling with high c_min\n",
    "    - Mixed batches: Strong discrimination via distribution statistics\n",
    "    \n",
    "    Args:\n",
    "        params (iterable): Iterable of parameters to optimize or dicts defining parameter groups\n",
    "        lr (float, optional): Learning rate. Default: 1e-3\n",
    "        betas (Tuple[float, float], optional): Coefficients for computing running averages \n",
    "            of gradient and its square (β₁, β₂). Default: (0.9, 0.999)\n",
    "        eps (float, optional): Term added to denominator for numerical stability. Default: 1e-8\n",
    "        weight_decay (float, optional): Decoupled weight decay coefficient (AdamW-style). \n",
    "            Default: 0.0\n",
    "        tau_init_steps (int, optional): Number of initial steps to collect AGAR samples \n",
    "            for automatic threshold calibration. Must be >= 50. Default: 500\n",
    "            Recommended: max(500, int(0.05 * total_steps))\n",
    "        tau_clip_range (Tuple[float, float], optional): Min/max bounds for tau threshold. \n",
    "            Default: (0.01, 0.5)\n",
    "        tau_dead_zone (float, optional): Dead zone factor for tau adaptation.\n",
    "            Ignores AGAR deviations smaller than this fraction of tau to prevent chasing noise.\n",
    "            Default: 0.2 (20%)\n",
    "        c_min (float, optional): Minimum confidence scaling factor to prevent learning rate \n",
    "            from becoming too small. Must be in [0, 1]. Default: 0.1\n",
    "            Note: After calibration, c_min is automatically computed based on noise level.\n",
    "        granularity (str, optional): AGAR computation granularity.\n",
    "            - 'parameter': Per-parameter confidence scaling (~13% overhead on large models).\n",
    "              Use for small models (<10M params) or when layer-specific adaptation matters.\n",
    "            - 'group': Per-group confidence scaling (faster than AdamW on large models).\n",
    "              Recommended for production use, large models (>10M params), and hyperparameter sweeps.\n",
    "            Default: 'group'\n",
    "        agar_clamp_factor (float, optional): Outlier clamping factor for AGAR computation.\n",
    "            Clamps moment estimates to ±(mean * factor) to handle extreme values.\n",
    "            Set to None to disable clamping. Default: 10.0\n",
    "        log_level (int, optional): Logging verbosity. 0=silent, 1=errors, 2=warnings, \n",
    "            3=info. Default: 1\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any parameter is outside its valid range\n",
    "        RuntimeError: If NaN or Inf gradients are detected during optimization\n",
    "        NotImplementedError: If sparse gradients are encountered\n",
    "    \n",
    "    Note:\n",
    "        This optimizer does not support sparse gradients. Use torch.optim.SparseAdam\n",
    "        for sparse gradient scenarios.\n",
    "    \n",
    "    Example:\n",
    "        >>> model = YourModel()\n",
    "        >>> optimizer = CASMO(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "        >>> \n",
    "        >>> for epoch in range(num_epochs):\n",
    "        ...     for batch in dataloader:\n",
    "        ...         optimizer.zero_grad()\n",
    "        ...         loss = model(batch)\n",
    "        ...         loss.backward()\n",
    "        ...         optimizer.step()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.0,\n",
    "        tau_init_steps: int = 500,\n",
    "        tau_clip_range: Tuple[float, float] = (0.01, 0.5),\n",
    "        tau_dead_zone: float = 0.2,  # Large dead zone to prevent chasing memorization\n",
    "        c_min: float = 0.1,\n",
    "        granularity: str = 'group',\n",
    "        agar_clamp_factor: Optional[float] = 10.0,\n",
    "        log_level: int = 1,\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta1: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta2: {betas[1]}\")\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(f\"Invalid weight_decay: {weight_decay}\")\n",
    "        if not 0.0 <= c_min <= 1.0:\n",
    "            raise ValueError(f\"Invalid c_min: {c_min}\")\n",
    "        if tau_init_steps < 50:\n",
    "            raise ValueError(f\"tau_init_steps too small: {tau_init_steps} (minimum: 50)\")\n",
    "        if not 0.0 <= tau_dead_zone <= 1.0:\n",
    "            raise ValueError(f\"Invalid tau_dead_zone: {tau_dead_zone} (must be in [0, 1])\")\n",
    "        if granularity not in ['parameter', 'group']:\n",
    "            raise ValueError(f\"Invalid granularity: {granularity} (must be 'parameter' or 'group')\")\n",
    "        \n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            tau_init_steps=tau_init_steps,\n",
    "            tau_clip_range=tau_clip_range,\n",
    "            tau_dead_zone=tau_dead_zone,\n",
    "            c_min=c_min,\n",
    "            granularity=granularity,\n",
    "            agar_clamp_factor=agar_clamp_factor,\n",
    "        )\n",
    "        \n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger('CASMO')\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(logging.Formatter('[CASMO] %(message)s'))\n",
    "            self.logger.addHandler(handler)\n",
    "        self.logger.setLevel(self._get_log_level(log_level))\n",
    "        \n",
    "        self._step_count = 0\n",
    "        \n",
    "        # Initialize per-group state for tau calibration and buffer reuse\n",
    "        self._group_states: Dict[int, Dict[str, Any]] = {}\n",
    "        for idx, group in enumerate(self.param_groups):\n",
    "            group_tau_dead_zone = group.get('tau_dead_zone', tau_dead_zone)\n",
    "            group_tau_clip_range = group.get('tau_clip_range', tau_clip_range)\n",
    "            group_tau_init_steps = group.get('tau_init_steps', tau_init_steps)\n",
    "            \n",
    "            self._group_states[idx] = {\n",
    "                'tau_adapter': DDEAdapter(1.0, group_tau_clip_range, dead_zone_factor=group_tau_dead_zone),\n",
    "                'tau_initialized': False,\n",
    "                'agar_buffer': deque(maxlen=group_tau_init_steps),\n",
    "                'reuse_buffer_exp_avg': None,\n",
    "                'reuse_buffer_exp_avg_sq': None,\n",
    "                'current_agar': None,\n",
    "                'current_confidence': None,\n",
    "                'agar_mean': None,\n",
    "                'agar_std': None,\n",
    "                'agar_median': None,\n",
    "                'agar_p10': None,\n",
    "                'agar_p90': None,\n",
    "                'c_min': c_min,\n",
    "            }\n",
    "    \n",
    "    def _get_log_level(self, level: int) -> int:\n",
    "        \"\"\"\n",
    "        Convert custom log level to Python logging level.\n",
    "        \n",
    "        Args:\n",
    "            level: Custom level (0=silent, 1=error, 2=warning, 3=info)\n",
    "            \n",
    "        Returns:\n",
    "            Python logging level constant\n",
    "        \"\"\"\n",
    "        level_map = {\n",
    "            0: logging.CRITICAL + 1,  # Silent\n",
    "            1: logging.ERROR,\n",
    "            2: logging.WARNING,\n",
    "            3: logging.INFO,\n",
    "        }\n",
    "        return level_map.get(level, logging.WARNING)\n",
    "    \n",
    "    def _log(self, level: int, message: str) -> None:\n",
    "        \"\"\"\n",
    "        Internal logging utility using Python logging module.\n",
    "        \n",
    "        Args:\n",
    "            level: Message severity level (1=error, 2=warning, 3=info)\n",
    "            message: Log message to output\n",
    "        \"\"\"\n",
    "        if level == 1:\n",
    "            self.logger.error(message)\n",
    "        elif level == 2:\n",
    "            self.logger.warning(message)\n",
    "        elif level == 3:\n",
    "            self.logger.info(message)\n",
    "    \n",
    "    def _validate_gradient(self, grad: torch.Tensor, group_idx: int) -> None:\n",
    "        \"\"\"\n",
    "        Validate gradient for NaN, Inf, and sparse tensors.\n",
    "        \n",
    "        Args:\n",
    "            grad: Gradient tensor to validate\n",
    "            group_idx: Parameter group index for error messages\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: If NaN or Inf detected\n",
    "            NotImplementedError: If sparse gradient detected\n",
    "        \"\"\"\n",
    "        if torch.isnan(grad).any():\n",
    "            raise RuntimeError(\n",
    "                f\"NaN gradient detected in parameter group {group_idx}. \"\n",
    "                \"Consider using gradient clipping: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\"\n",
    "            )\n",
    "        if torch.isinf(grad).any():\n",
    "            raise RuntimeError(\n",
    "                f\"Inf gradient detected in parameter group {group_idx}. \"\n",
    "                \"Check for numerical overflow in loss computation or model outputs.\"\n",
    "            )\n",
    "        if grad.is_sparse:\n",
    "            raise NotImplementedError(\n",
    "                \"CASMO does not support sparse gradients. \"\n",
    "                \"Use torch.optim.SparseAdam for sparse gradient scenarios, \"\n",
    "                \"or convert gradients to dense format with grad.to_dense().\"\n",
    "            )\n",
    "    \n",
    "    def _init_param_state(self, p: torch.Tensor) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Initialize optimizer state for a parameter.\n",
    "        \n",
    "        Args:\n",
    "            p: Parameter tensor\n",
    "            \n",
    "        Returns:\n",
    "            Initialized state dictionary with step counter and moment estimates\n",
    "        \"\"\"\n",
    "        state = self.state[p]\n",
    "        if len(state) == 0:\n",
    "            state['step'] = 0\n",
    "            state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "            state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "    \n",
    "    def _update_moments(self, state: Dict[str, Any], grad: torch.Tensor, beta1: float, beta2: float) -> None:\n",
    "        \"\"\"\n",
    "        Update exponential moving averages of gradient moments.\n",
    "        \n",
    "        Args:\n",
    "            state: Parameter state dictionary\n",
    "            grad: Current gradient\n",
    "            beta1: First moment decay rate (β₁)\n",
    "            beta2: Second moment decay rate (β₂)\n",
    "        \"\"\"\n",
    "        exp_avg = state['exp_avg']\n",
    "        exp_avg_sq = state['exp_avg_sq']\n",
    "        state['step'] += 1\n",
    "        \n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "    \n",
    "    def _apply_weight_update(self, p: torch.Tensor, state: Dict[str, Any], lr: float, \n",
    "                            weight_decay: float, eps: float, confidence: torch.Tensor,\n",
    "                            beta1: float, beta2: float) -> None:\n",
    "        \"\"\"\n",
    "        Apply Adam-style parameter update with confidence-scaled learning rate.\n",
    "        \n",
    "        Implements decoupled weight decay (AdamW) with bias-corrected moments\n",
    "        and confidence-based learning rate modulation.\n",
    "        \n",
    "        Args:\n",
    "            p: Parameter tensor to update\n",
    "            state: Parameter state dictionary containing moments\n",
    "            lr: Base learning rate\n",
    "            weight_decay: Decoupled weight decay coefficient\n",
    "            eps: Numerical stability constant (ε)\n",
    "            confidence: Confidence scaling factor in [c_min, 1.0]\n",
    "            beta1: First moment decay rate (β₁)\n",
    "            beta2: Second moment decay rate (β₂)\n",
    "        \"\"\"\n",
    "        exp_avg = state['exp_avg']\n",
    "        exp_avg_sq = state['exp_avg_sq']\n",
    "        step = state['step']\n",
    "        \n",
    "        # Bias correction\n",
    "        bias_correction1 = 1 - beta1 ** step\n",
    "        bias_correction2 = 1 - beta2 ** step\n",
    "        m_hat = exp_avg / bias_correction1\n",
    "        v_hat = exp_avg_sq / bias_correction2\n",
    "        \n",
    "        # Weight decay (decoupled)\n",
    "        if weight_decay != 0:\n",
    "            p.mul_(1 - lr * weight_decay)\n",
    "        \n",
    "        # Apply update with confidence-scaled learning rate\n",
    "        denom = v_hat.sqrt().add_(eps)\n",
    "        step_size = lr * confidence\n",
    "        p.addcdiv_(m_hat, denom, value=-step_size)\n",
    "    \n",
    "    def _compute_agar(\n",
    "        self,\n",
    "        exp_avg: torch.Tensor,\n",
    "        exp_avg_sq: torch.Tensor,\n",
    "        eps: float,\n",
    "        clamp_factor: Optional[float],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Adaptive Gradient Alignment Ratio (AGAR) from exponential moving averages.\n",
    "        \n",
    "        AGAR quantifies the signal-to-noise ratio in gradients:\n",
    "            AGAR = mean(signal / (signal + noise))\n",
    "            where signal = (E[g])² (squared mean gradient per element)\n",
    "                  noise = Var[g] = E[g²] - (E[g])² (gradient variance per element)\n",
    "        \n",
    "        Args:\n",
    "            exp_avg (torch.Tensor): Exponential moving average of gradients (first moment)\n",
    "            exp_avg_sq (torch.Tensor): Exponential moving average of squared gradients (second moment)\n",
    "            eps (float): Small constant for numerical stability\n",
    "            clamp_factor (Optional[float]): Outlier clamping factor (None to disable)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Scalar AGAR value in range [0, 1], where:\n",
    "                - 0 indicates pure noise (no consistent gradient direction)\n",
    "                - 1 indicates pure signal (perfectly consistent gradients)\n",
    "        \n",
    "        Note:\n",
    "            AGAR is computed per-element then uniformly averaged across all elements.\n",
    "            This provides robustness across parameters with different scales.\n",
    "            Uses raw moments to preserve the variance relationship Var[g] = E[g²] - (E[g])².\n",
    "            Bias correction would distort this relationship and cause AGAR instability.\n",
    "        \"\"\"\n",
    "        # Outlier protection: clamp extreme values based on gradient statistics\n",
    "        if clamp_factor is not None:\n",
    "            m_scale = exp_avg.abs().mean() + eps\n",
    "            v_scale = exp_avg_sq.mean() + eps\n",
    "            m_clamped = torch.clamp(exp_avg, min=-m_scale * clamp_factor, max=m_scale * clamp_factor)\n",
    "            v_clamped = torch.clamp(exp_avg_sq, min=0.0, max=v_scale * clamp_factor)\n",
    "        else:\n",
    "            m_clamped = exp_avg\n",
    "            v_clamped = exp_avg_sq\n",
    "        \n",
    "        # Signal: squared norm of mean gradient (consistent direction)\n",
    "        signal_per_elem = m_clamped.pow(2)\n",
    "        \n",
    "        # Noise: gradient variance = E[g²] - (E[g])²\n",
    "        noise_per_elem = torch.clamp(v_clamped - signal_per_elem, min=eps)\n",
    "        \n",
    "        # Compute mean AGAR across all elements (uniform weighting)\n",
    "        agar_per_elem = signal_per_elem / (signal_per_elem + noise_per_elem + eps)\n",
    "        agar = agar_per_elem.mean()\n",
    "        \n",
    "        return torch.clamp(agar, min=0.0, max=1.0)\n",
    "    \n",
    "    # Calibration constants\n",
    "    MIN_CALIBRATION_SAMPLES = 50\n",
    "    MIN_STD_THRESHOLD = 0.01  # Prevent division by zero\n",
    "    \n",
    "    # Coefficient of variation thresholds for adaptive c_min\n",
    "    CV_HIGH_THRESHOLD = 0.5  # Bimodal distribution\n",
    "    CV_MEDIUM_THRESHOLD = 0.3  # Some separation\n",
    "    \n",
    "    # Adaptive c_min values\n",
    "    C_MIN_HIGH_VARIANCE = 0.1  # Strong discrimination for bimodal\n",
    "    C_MIN_MEDIUM_VARIANCE = 0.3  # Moderate discrimination\n",
    "    C_MIN_LOW_VARIANCE = 0.5  # High baseline for unimodal/pervasive noise\n",
    "    \n",
    "    def _calibrate_tau(self, agar_buffer: deque, tau_clip_range: Tuple[float, float], group_idx: int) -> float:\n",
    "        \"\"\"\n",
    "        Universal tau calibration using distribution statistics.\n",
    "        \n",
    "        Computes distribution parameters for confidence mapping:\n",
    "        - μ (mean): Central tendency of AGAR distribution\n",
    "        - σ (std): Spread of AGAR distribution\n",
    "        - p50 (median): Robust center estimate\n",
    "        - p10, p90: Distribution bounds for outlier detection\n",
    "        \n",
    "        This approach works universally for:\n",
    "        - Clean data: High μ, low σ → High confidence baseline\n",
    "        - Pervasive noise: Low μ, low σ → Adaptive confidence scaling\n",
    "        - Mixed batches: Medium μ, high σ → Bimodal confidence distribution\n",
    "        \n",
    "        Mathematical foundation:\n",
    "            confidence(agar) = c_min + (1 - c_min) * sigmoid((agar - μ) / σ)\n",
    "        \n",
    "        This sigmoid mapping naturally adapts to any distribution shape.\n",
    "        \n",
    "        Args:\n",
    "            agar_buffer: Collection of AGAR samples from initial training steps\n",
    "            tau_clip_range: Safety bounds for tau (min, max)\n",
    "            group_idx: Parameter group index for storing calibration results\n",
    "        \n",
    "        Returns:\n",
    "            Calibrated tau threshold (median for robustness)\n",
    "        \"\"\"\n",
    "        if len(agar_buffer) < self.MIN_CALIBRATION_SAMPLES:\n",
    "            return tau_clip_range[1]\n",
    "        \n",
    "        samples = np.array(agar_buffer)\n",
    "        \n",
    "        # Distribution statistics\n",
    "        mu = np.mean(samples)\n",
    "        sigma = np.std(samples)\n",
    "        median = np.median(samples)\n",
    "        p10 = np.percentile(samples, 10)\n",
    "        p90 = np.percentile(samples, 90)\n",
    "        \n",
    "        # Store distribution parameters for confidence mapping\n",
    "        group_state = self._group_states[group_idx]\n",
    "        group_state['agar_mean'] = float(mu)\n",
    "        group_state['agar_std'] = float(max(sigma, self.MIN_STD_THRESHOLD))\n",
    "        group_state['agar_median'] = float(median)\n",
    "        group_state['agar_p10'] = float(p10)\n",
    "        group_state['agar_p90'] = float(p90)\n",
    "        \n",
    "        # Adaptive c_min based on coefficient of variation (CV = σ/μ)\n",
    "        # High CV → Lower c_min (strong discrimination for bimodal distributions)\n",
    "        # Low CV → Higher c_min (prevent over-suppression for unimodal/pervasive noise)\n",
    "        cv = sigma / (mu + 1e-8)\n",
    "        if cv > self.CV_HIGH_THRESHOLD:\n",
    "            c_min_adaptive = self.C_MIN_HIGH_VARIANCE\n",
    "        elif cv > self.CV_MEDIUM_THRESHOLD:\n",
    "            c_min_adaptive = self.C_MIN_MEDIUM_VARIANCE\n",
    "        else:\n",
    "            c_min_adaptive = self.C_MIN_LOW_VARIANCE\n",
    "        \n",
    "        group_state['c_min'] = float(c_min_adaptive)\n",
    "        \n",
    "        self._log(2, f\"Calibrated AGAR distribution: μ={mu:.4f}, σ={sigma:.4f}, \"\n",
    "                     f\"median={median:.4f}, CV={cv:.4f}, c_min={c_min_adaptive:.2f}\")\n",
    "        \n",
    "        # Return median as tau (robust to outliers)\n",
    "        return float(np.clip(median, tau_clip_range[0], tau_clip_range[1]))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Optional[Callable] = None) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Perform a single optimization step.\n",
    "        \n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model and returns\n",
    "                the loss. Optional for most optimizers but required for some (e.g., LBFGS).\n",
    "        \n",
    "        Returns:\n",
    "            Optional[float]: Loss value if closure is provided, None otherwise\n",
    "        \n",
    "        Raises:\n",
    "            RuntimeError: If NaN or Inf gradients are detected\n",
    "            NotImplementedError: If sparse gradients are encountered\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        \n",
    "        self._step_count += 1\n",
    "        \n",
    "        for group_idx, group in enumerate(self.param_groups):\n",
    "            beta1, beta2 = group['betas']\n",
    "            lr = group['lr']\n",
    "            eps = group['eps']\n",
    "            weight_decay = group['weight_decay']\n",
    "            c_min = group['c_min']\n",
    "            tau_init_steps = group['tau_init_steps']\n",
    "            tau_clip_range = group['tau_clip_range']\n",
    "            granularity = group['granularity']\n",
    "            agar_clamp_factor = group['agar_clamp_factor']\n",
    "            \n",
    "            group_state = self._group_states[group_idx]\n",
    "            \n",
    "            # Per-group AGAR mode: compute once for all parameters in group\n",
    "            if granularity == 'group':\n",
    "                # Skip group if no parameters have gradients\n",
    "                valid_params = [p for p in group['params'] if p.grad is not None]\n",
    "                if not valid_params:\n",
    "                    continue\n",
    "                \n",
    "                all_exp_avg = []\n",
    "                all_exp_avg_sq = []\n",
    "                \n",
    "                # First pass: update momentum and collect states\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    \n",
    "                    self._validate_gradient(p.grad, group_idx)\n",
    "                    state = self._init_param_state(p)\n",
    "                    self._update_moments(state, p.grad, beta1, beta2)\n",
    "                    \n",
    "                    all_exp_avg.append(state['exp_avg'].flatten())\n",
    "                    all_exp_avg_sq.append(state['exp_avg_sq'].flatten())\n",
    "                \n",
    "                # Compute group-level AGAR using pre-allocated buffers\n",
    "                if all_exp_avg:\n",
    "                    # Allocate buffers on first use (amortized across all steps)\n",
    "                    if group_state['reuse_buffer_exp_avg'] is None:\n",
    "                        total_params = sum(m.numel() for m in all_exp_avg)\n",
    "                        device = all_exp_avg[0].device\n",
    "                        dtype = all_exp_avg[0].dtype\n",
    "                        group_state['reuse_buffer_exp_avg'] = torch.zeros(total_params, device=device, dtype=dtype)\n",
    "                        group_state['reuse_buffer_exp_avg_sq'] = torch.zeros(total_params, device=device, dtype=dtype)\n",
    "                    \n",
    "                    # Copy moment estimates into buffers (avoids repeated allocations)\n",
    "                    offset = 0\n",
    "                    reuse_buffer_exp_avg = group_state['reuse_buffer_exp_avg']\n",
    "                    reuse_buffer_exp_avg_sq = group_state['reuse_buffer_exp_avg_sq']\n",
    "                    \n",
    "                    for m, v in zip(all_exp_avg, all_exp_avg_sq):\n",
    "                        numel = m.numel()\n",
    "                        reuse_buffer_exp_avg[offset:offset+numel].copy_(m)\n",
    "                        reuse_buffer_exp_avg_sq[offset:offset+numel].copy_(v)\n",
    "                        offset += numel\n",
    "                    \n",
    "                    # Compute AGAR on concatenated moments\n",
    "                    agar = self._compute_agar(\n",
    "                        reuse_buffer_exp_avg[:offset],\n",
    "                        reuse_buffer_exp_avg_sq[:offset],\n",
    "                        eps,\n",
    "                        agar_clamp_factor\n",
    "                    )\n",
    "                    \n",
    "                    agar_value = agar.item()\n",
    "                    group_state['current_agar'] = agar_value\n",
    "                    \n",
    "                    # Tau calibration and adaptation\n",
    "                    if not group_state['tau_initialized']:\n",
    "                        group_state['agar_buffer'].append(agar_value)\n",
    "                        \n",
    "                        # Diagnostic logging during calibration\n",
    "                        if self._step_count % 10 == 0 and len(group_state['agar_buffer']) > 0:\n",
    "                            agars = list(group_state['agar_buffer'])\n",
    "                            self._log(3, f\"Step {self._step_count} - AGAR: min={min(agars):.4f}, median={np.median(agars):.4f}, max={max(agars):.4f}\")\n",
    "                        \n",
    "                        if len(group_state['agar_buffer']) >= tau_init_steps:\n",
    "                            tau = self._calibrate_tau(group_state['agar_buffer'], tau_clip_range, group_idx)\n",
    "                            group_state['tau_adapter'].tau = tau\n",
    "                            group_state['tau_adapter'].tau_calibrated = tau  # Anchor dead zone to calibrated value\n",
    "                            group_state['tau_initialized'] = True\n",
    "                            group_state['agar_buffer'].clear()\n",
    "                            self._log(2, f\"Group {group_idx}: Tau calibrated to {tau:.4f} from {tau_init_steps} samples\")\n",
    "                    else:\n",
    "                        # Post-calibration: adapt tau using drift-detecting EMA\n",
    "                        group_state['tau_adapter'].update(agar_value)\n",
    "                    \n",
    "                    # Universal sigmoid-based confidence mapping\n",
    "                    if group_state['tau_initialized']:\n",
    "                        mu = group_state.get('agar_mean', agar_value)\n",
    "                        sigma = group_state.get('agar_std', 0.1)\n",
    "                        c_min_adaptive = group_state.get('c_min', c_min)\n",
    "                        \n",
    "                        # Sigmoid mapping: confidence = c_min + (1 - c_min) * sigmoid((agar - μ) / σ)\n",
    "                        # This naturally adapts to any distribution:\n",
    "                        # - High μ, low σ (clean): Most samples get high confidence\n",
    "                        # - Low μ, low σ (pervasive noise): Confidence scales smoothly from c_min\n",
    "                        # - High σ (mixed): Strong discrimination between low/high AGAR\n",
    "                        z_score = (agar_value - mu) / sigma\n",
    "                        sigmoid = 1.0 / (1.0 + np.exp(-z_score))\n",
    "                        confidence_value = c_min_adaptive + (1.0 - c_min_adaptive) * sigmoid\n",
    "                        \n",
    "                        confidence_value = float(np.clip(confidence_value, c_min_adaptive, 1.0))\n",
    "                    else:\n",
    "                        # Pre-calibration: simple passthrough\n",
    "                        confidence_value = float(np.clip(agar_value, c_min, 1.0))\n",
    "                    \n",
    "                    group_state['current_confidence'] = confidence_value\n",
    "                    \n",
    "                    # Diagnostic logging\n",
    "                    if group_state['tau_initialized'] and self._step_count % 100 == 0:\n",
    "                        mu = group_state.get('agar_mean', 0)\n",
    "                        sigma = group_state.get('agar_std', 0)\n",
    "                        self._log(3, f\"Step {self._step_count} - AGAR={agar_value:.4f}, μ={mu:.4f}, \"\n",
    "                                     f\"σ={sigma:.4f}, Confidence={confidence_value:.4f}\")\n",
    "                    \n",
    "                    confidence_tensor = torch.tensor(confidence_value, device=all_exp_avg[0].device, dtype=all_exp_avg[0].dtype)\n",
    "                else:\n",
    "                    confidence_tensor = torch.tensor(c_min)\n",
    "                \n",
    "                # Apply parameter updates with confidence-scaled learning rate\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    \n",
    "                    self._apply_weight_update(p, self.state[p], lr, weight_decay, \n",
    "                                             eps, confidence_tensor, beta1, beta2)\n",
    "            \n",
    "            # Per-parameter AGAR mode: compute separately for each parameter\n",
    "            else:\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    \n",
    "                    self._validate_gradient(p.grad, group_idx)\n",
    "                    state = self._init_param_state(p)\n",
    "                    self._update_moments(state, p.grad, beta1, beta2)\n",
    "                    \n",
    "                    # Compute per-parameter AGAR\n",
    "                    agar = self._compute_agar(state['exp_avg'], state['exp_avg_sq'], eps, agar_clamp_factor)\n",
    "                    \n",
    "                    agar_value = agar.item()\n",
    "                    \n",
    "                    # Store first parameter's AGAR as representative for monitoring\n",
    "                    if group_state['current_agar'] is None:\n",
    "                        group_state['current_agar'] = agar_value\n",
    "                    \n",
    "                    # Tau calibration and adaptation\n",
    "                    if not group_state['tau_initialized']:\n",
    "                        group_state['agar_buffer'].append(agar_value)\n",
    "                        \n",
    "                        # Diagnostic logging during calibration\n",
    "                        if self._step_count % 10 == 0 and len(group_state['agar_buffer']) > 0:\n",
    "                            agars = list(group_state['agar_buffer'])\n",
    "                            self._log(3, f\"Step {self._step_count} - AGAR: min={min(agars):.4f}, median={np.median(agars):.4f}, max={max(agars):.4f}\")\n",
    "                        \n",
    "                        if len(group_state['agar_buffer']) >= tau_init_steps:\n",
    "                            tau = self._calibrate_tau(group_state['agar_buffer'], tau_clip_range, group_idx)\n",
    "                            group_state['tau_adapter'].tau = tau\n",
    "                            group_state['tau_adapter'].tau_calibrated = tau  # Anchor dead zone to calibrated value\n",
    "                            group_state['tau_initialized'] = True\n",
    "                            group_state['agar_buffer'].clear()\n",
    "                            self._log(2, f\"Group {group_idx}: Tau calibrated to {tau:.4f} from {tau_init_steps} samples\")\n",
    "                    else:\n",
    "                        # Post-calibration: adapt tau using drift-detecting EMA\n",
    "                        group_state['tau_adapter'].update(agar_value)\n",
    "                    \n",
    "                    # Universal sigmoid-based confidence mapping\n",
    "                    if group_state['tau_initialized']:\n",
    "                        mu = group_state.get('agar_mean', agar_value)\n",
    "                        sigma = group_state.get('agar_std', 0.1)\n",
    "                        c_min_adaptive = group_state.get('c_min', c_min)\n",
    "                        \n",
    "                        # Sigmoid mapping: confidence = c_min + (1 - c_min) * sigmoid((agar - μ) / σ)\n",
    "                        z_score = (agar_value - mu) / sigma\n",
    "                        sigmoid = 1.0 / (1.0 + np.exp(-z_score))\n",
    "                        confidence_value = c_min_adaptive + (1.0 - c_min_adaptive) * sigmoid\n",
    "                        \n",
    "                        confidence_value = float(np.clip(confidence_value, c_min_adaptive, 1.0))\n",
    "                    else:\n",
    "                        # Pre-calibration: simple passthrough\n",
    "                        confidence_value = float(np.clip(agar_value, c_min, 1.0))\n",
    "                    \n",
    "                    confidence_tensor = torch.tensor(confidence_value, device=p.device, dtype=p.dtype)\n",
    "                    \n",
    "                    # Store first parameter's confidence as representative for monitoring\n",
    "                    if group_state['current_confidence'] is None:\n",
    "                        group_state['current_confidence'] = confidence_value\n",
    "                    \n",
    "                    # Diagnostic logging\n",
    "                    if group_state['tau_initialized'] and self._step_count % 100 == 0:\n",
    "                        mu = group_state.get('agar_mean', 0)\n",
    "                        sigma = group_state.get('agar_std', 0)\n",
    "                        self._log(3, f\"Step {self._step_count} - AGAR={agar_value:.4f}, μ={mu:.4f}, \"\n",
    "                                     f\"σ={sigma:.4f}, Confidence={confidence_value:.4f}\")\n",
    "                    \n",
    "                    # Apply parameter update\n",
    "                    self._apply_weight_update(p, state, lr, weight_decay, \n",
    "                                             eps, confidence_tensor, beta1, beta2)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def state_dict(self):\n",
    "        \"\"\"\n",
    "        Return the optimizer state as a dictionary.\n",
    "        \n",
    "        Includes all parameter states, hyperparameters, and internal calibration data.\n",
    "        Compatible with torch.save() for checkpointing.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Complete optimizer state including:\n",
    "                - Parameter-specific states (exp_avg, exp_avg_sq, step)\n",
    "                - Group-level calibration data (tau, agar_buffer)\n",
    "                - Global step counter\n",
    "        \"\"\"\n",
    "        state_dict = super().state_dict()\n",
    "        \n",
    "        # Serialize group states (convert deque to list)\n",
    "        serializable_group_states = {}\n",
    "        for idx, gs in self._group_states.items():\n",
    "            serializable_group_states[idx] = {\n",
    "                'tau_initialized': gs['tau_initialized'],\n",
    "                'agar_buffer': list(gs['agar_buffer']),\n",
    "                'agar_buffer_maxlen': gs['agar_buffer'].maxlen,\n",
    "                'adapter_tau': gs['tau_adapter'].tau,\n",
    "                'adapter_tau_calibrated': gs['tau_adapter'].tau_calibrated,\n",
    "                'adapter_mean': gs['tau_adapter'].mean_agar,\n",
    "                'adapter_var': gs['tau_adapter'].ema_var,\n",
    "                'agar_mean': gs.get('agar_mean'),\n",
    "                'agar_std': gs.get('agar_std'),\n",
    "                'agar_median': gs.get('agar_median'),\n",
    "                'agar_p10': gs.get('agar_p10'),\n",
    "                'agar_p90': gs.get('agar_p90'),\n",
    "                'c_min': gs.get('c_min'),\n",
    "            }\n",
    "        \n",
    "        state_dict['_group_states'] = serializable_group_states\n",
    "        state_dict['_step_count'] = self._step_count\n",
    "        \n",
    "        return state_dict\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"\n",
    "        Load optimizer state from a dictionary.\n",
    "        \n",
    "        Restores all parameter states, hyperparameters, and internal calibration data.\n",
    "        Compatible with torch.load() for checkpoint restoration.\n",
    "        \n",
    "        Args:\n",
    "            state_dict (dict): Optimizer state dictionary (typically from state_dict())\n",
    "        \n",
    "        Note:\n",
    "            Automatically handles conversion of serialized lists back to deque objects\n",
    "            for AGAR buffer management.\n",
    "        \"\"\"\n",
    "        # Restore group states (convert list back to deque)\n",
    "        if '_group_states' in state_dict:\n",
    "            loaded_states = state_dict.pop('_group_states')\n",
    "            # Ensure keys are integers (they may be strings after JSON serialization)\n",
    "            self._group_states = {}\n",
    "            for idx, gs in loaded_states.items():\n",
    "                idx_int = int(idx) if isinstance(idx, str) else idx\n",
    "                \n",
    "                maxlen = gs.pop('agar_buffer_maxlen', None)\n",
    "                buffer_list = gs.pop('agar_buffer', [])\n",
    "                gs['agar_buffer'] = deque(buffer_list, maxlen=maxlen)\n",
    "                \n",
    "                # Restore adapter state\n",
    "                tau_clip_range = self.param_groups[idx_int]['tau_clip_range']\n",
    "                tau_dead_zone = self.param_groups[idx_int]['tau_dead_zone']\n",
    "                adapter = DDEAdapter(1.0, tau_clip_range, dead_zone_factor=tau_dead_zone)\n",
    "                adapter.tau = gs.pop('adapter_tau', 1.0)\n",
    "                adapter.tau_calibrated = gs.pop('adapter_tau_calibrated', None)\n",
    "                adapter.mean_agar = gs.pop('adapter_mean', 1.0)\n",
    "                adapter.ema_var = gs.pop('adapter_var', 0.01)\n",
    "                gs['tau_adapter'] = adapter\n",
    "                \n",
    "                # Initialize missing fields\n",
    "                gs.setdefault('reuse_buffer_exp_avg', None)\n",
    "                gs.setdefault('reuse_buffer_exp_avg_sq', None)\n",
    "                gs.setdefault('current_agar', None)\n",
    "                gs.setdefault('current_confidence', None)\n",
    "                gs.setdefault('agar_mean', None)\n",
    "                gs.setdefault('agar_std', None)\n",
    "                gs.setdefault('agar_median', None)\n",
    "                gs.setdefault('agar_p10', None)\n",
    "                gs.setdefault('agar_p90', None)\n",
    "                gs.setdefault('c_min', self.param_groups[idx_int].get('c_min', 0.1))\n",
    "                \n",
    "                self._group_states[idx_int] = gs\n",
    "        \n",
    "        # Restore step count\n",
    "        if '_step_count' in state_dict:\n",
    "            self._step_count = state_dict.pop('_step_count')\n",
    "        \n",
    "        super().load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52454345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CASMO vs AdamW Benchmark - Long-Tailed CIFAR-100\n",
      "Imbalance Factor: 100:1\n",
      "======================================================================\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:05<00:00, 30.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "  Total training samples: 10847\n",
      "  Many-shot classes (>100 samples): 35\n",
      "  Medium-shot classes (20-100 samples): 35\n",
      "  Few-shot classes (<20 samples): 30\n",
      "\n",
      "Starting CASMO run...\n",
      "\n",
      "======================================================================\n",
      "Running: CASMO\n",
      "======================================================================\n",
      "\n",
      "Epoch 20/200 | Train: 36.6% | Val: 15.8% | Many: 34.3% | Medium: 9.7% | Few: 1.2%\n",
      "Epoch 40/200 | Train: 57.2% | Val: 24.8% | Many: 48.1% | Medium: 19.0% | Few: 4.6%\n",
      "Epoch 60/200 | Train: 71.6% | Val: 25.4% | Many: 48.1% | Medium: 21.5% | Few: 3.7%\n",
      "Epoch 80/200 | Train: 82.3% | Val: 27.8% | Many: 51.8% | Medium: 23.5% | Few: 4.9%\n",
      "Epoch 100/200 | Train: 88.3% | Val: 27.0% | Many: 50.6% | Medium: 21.5% | Few: 5.7%\n",
      "Epoch 120/200 | Train: 93.8% | Val: 28.2% | Many: 51.3% | Medium: 24.9% | Few: 5.2%\n",
      "Epoch 140/200 | Train: 96.8% | Val: 28.2% | Many: 52.4% | Medium: 22.2% | Few: 6.9%\n",
      "Epoch 160/200 | Train: 98.3% | Val: 28.9% | Many: 53.2% | Medium: 24.2% | Few: 6.1%\n",
      "Epoch 180/200 | Train: 99.2% | Val: 29.1% | Many: 53.1% | Medium: 24.7% | Few: 6.1%\n",
      "Epoch 200/200 | Train: 99.5% | Val: 28.9% | Many: 53.5% | Medium: 24.0% | Few: 5.9%\n",
      "Finished in 2824.0s\n",
      "\n",
      "======================================================================\n",
      "Final AGAR Statistics (last 100 steps):\n",
      "======================================================================\n",
      "  Mean: 0.0059\n",
      "  Std:  0.0017\n",
      "  Min:  0.0029\n",
      "  Max:  0.0123\n",
      "\n",
      "======================================================================\n",
      "Final Confidence Statistics (last 100 steps):\n",
      "======================================================================\n",
      "  Mean: 0.4988\n",
      "  Std:  0.0069\n",
      "  Min:  0.4868\n",
      "  Max:  0.5245\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Starting AdamW run...\n",
      "\n",
      "======================================================================\n",
      "Running: ADAMW\n",
      "======================================================================\n",
      "\n",
      "Epoch 20/200 | Train: 25.2% | Val: 9.7% | Many: 23.9% | Medium: 3.6% | Few: 0.3%\n",
      "Epoch 40/200 | Train: 35.2% | Val: 15.2% | Many: 34.4% | Medium: 7.9% | Few: 1.4%\n",
      "Epoch 60/200 | Train: 42.6% | Val: 16.2% | Many: 36.4% | Medium: 9.1% | Few: 0.9%\n",
      "Epoch 80/200 | Train: 49.1% | Val: 20.9% | Many: 43.2% | Medium: 14.1% | Few: 2.8%\n",
      "Epoch 100/200 | Train: 54.3% | Val: 21.0% | Many: 42.6% | Medium: 15.2% | Few: 2.6%\n",
      "Epoch 120/200 | Train: 59.5% | Val: 21.8% | Many: 42.9% | Medium: 17.3% | Few: 2.6%\n",
      "Epoch 140/200 | Train: 63.7% | Val: 22.7% | Many: 45.4% | Medium: 16.5% | Few: 3.4%\n",
      "Epoch 160/200 | Train: 66.6% | Val: 23.5% | Many: 46.9% | Medium: 17.2% | Few: 3.5%\n",
      "Epoch 180/200 | Train: 69.0% | Val: 23.5% | Many: 45.9% | Medium: 17.9% | Few: 4.0%\n",
      "Epoch 200/200 | Train: 69.8% | Val: 23.9% | Many: 46.6% | Medium: 18.2% | Few: 4.1%\n",
      "Finished in 2325.4s\n",
      "\n",
      "Plotting results...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-837857967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-837857967.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;31m# Create results directory if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m     \u001b[0mresults_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Long-Tailed Recognition Benchmark: CIFAR-100 with Class Imbalance\n",
    "\n",
    "Tests CASMO's ability to handle imbalanced datasets where standard optimizers\n",
    "overfit to majority classes and ignore minority (tail) classes.\n",
    "\n",
    "Task:\n",
    "    Dataset: CIFAR-100 with exponential long-tail distribution\n",
    "    Imbalance Ratio: 100:1 (most frequent class has 100x samples of rarest)\n",
    "    \n",
    "    Hypothesis:\n",
    "    - AdamW will overfit to majority classes (high head accuracy, low tail accuracy).\n",
    "    - CASMO will balance learning across all classes (better tail accuracy).\n",
    "    \n",
    "    Reasoning:\n",
    "    - Majority class gradients: Consistent (high AGAR) → normal learning rate\n",
    "    - Minority class gradients: Noisy due to few samples (low AGAR) → CASMO prevents overfitting\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import random\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Dataset: Long-Tailed CIFAR-100\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class LongTailCIFAR100(Dataset):\n",
    "    \"\"\"\n",
    "    CIFAR-100 with exponential long-tail distribution.\n",
    "    \n",
    "    Creates imbalanced dataset where:\n",
    "    - Head classes (many-shot): >100 samples\n",
    "    - Medium classes (medium-shot): 20-100 samples  \n",
    "    - Tail classes (few-shot): <20 samples\n",
    "    \"\"\"\n",
    "    def __init__(self, root='./data', train=True, imbalance_factor=100, seed=42):\n",
    "        self.train = train\n",
    "        self.imbalance_factor = imbalance_factor\n",
    "        \n",
    "        # Load full CIFAR-100\n",
    "        self.dataset = torchvision.datasets.CIFAR100(\n",
    "            root=root, \n",
    "            train=train, \n",
    "            download=True,\n",
    "            transform=None  # We'll apply transforms later\n",
    "        )\n",
    "        \n",
    "        if train:\n",
    "            # Create long-tail distribution\n",
    "            self.data, self.targets, self.class_counts = self._create_long_tail(seed)\n",
    "        else:\n",
    "            # Keep test set balanced\n",
    "            self.data = self.dataset.data\n",
    "            self.targets = self.dataset.targets\n",
    "            self.class_counts = self._count_classes()\n",
    "        \n",
    "        # Define transforms\n",
    "        if train:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "            ])\n",
    "    \n",
    "    def _create_long_tail(self, seed):\n",
    "        \"\"\"Create exponentially imbalanced dataset.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        num_classes = 100\n",
    "        # Get samples per class in original dataset\n",
    "        original_data = np.array(self.dataset.data)\n",
    "        original_targets = np.array(self.dataset.targets)\n",
    "        \n",
    "        # Calculate target samples per class (exponential decay)\n",
    "        max_samples = 500  # Maximum samples for most frequent class\n",
    "        samples_per_class = []\n",
    "        for i in range(num_classes):\n",
    "            # Exponential decay: n_i = n_max * (imb_factor)^(-i/(C-1))\n",
    "            n_samples = int(max_samples * (self.imbalance_factor ** (-i / (num_classes - 1))))\n",
    "            samples_per_class.append(max(n_samples, 5))  # At least 5 samples per class\n",
    "        \n",
    "        # Sample from each class\n",
    "        new_data = []\n",
    "        new_targets = []\n",
    "        class_counts = {}\n",
    "        \n",
    "        for class_idx in range(num_classes):\n",
    "            class_mask = original_targets == class_idx\n",
    "            class_data = original_data[class_mask]\n",
    "            \n",
    "            n_samples = samples_per_class[class_idx]\n",
    "            n_available = len(class_data)\n",
    "            \n",
    "            if n_samples > n_available:\n",
    "                # Oversample if needed\n",
    "                indices = np.random.choice(n_available, n_samples, replace=True)\n",
    "            else:\n",
    "                # Subsample\n",
    "                indices = np.random.choice(n_available, n_samples, replace=False)\n",
    "            \n",
    "            sampled_data = class_data[indices]\n",
    "            new_data.append(sampled_data)\n",
    "            new_targets.extend([class_idx] * n_samples)\n",
    "            class_counts[class_idx] = n_samples\n",
    "        \n",
    "        new_data = np.concatenate(new_data, axis=0)\n",
    "        new_targets = np.array(new_targets)\n",
    "        \n",
    "        # Shuffle\n",
    "        shuffle_idx = np.random.permutation(len(new_targets))\n",
    "        new_data = new_data[shuffle_idx]\n",
    "        new_targets = new_targets[shuffle_idx]\n",
    "        \n",
    "        return new_data, new_targets.tolist(), class_counts\n",
    "    \n",
    "    def _count_classes(self):\n",
    "        \"\"\"Count samples per class.\"\"\"\n",
    "        class_counts = {}\n",
    "        for target in self.targets:\n",
    "            class_counts[target] = class_counts.get(target, 0) + 1\n",
    "        return class_counts\n",
    "    \n",
    "    def get_class_groups(self):\n",
    "        \"\"\"Categorize classes into many/medium/few shot.\"\"\"\n",
    "        many_shot = []  # >100 samples\n",
    "        medium_shot = []  # 20-100 samples\n",
    "        few_shot = []  # <20 samples\n",
    "        \n",
    "        for class_idx, count in self.class_counts.items():\n",
    "            if count > 100:\n",
    "                many_shot.append(class_idx)\n",
    "            elif count >= 20:\n",
    "                medium_shot.append(class_idx)\n",
    "            else:\n",
    "                few_shot.append(class_idx)\n",
    "        \n",
    "        return many_shot, medium_shot, few_shot\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Model: ResNet-32 for CIFAR-100\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic residual block for ResNet.\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet for CIFAR-100.\"\"\"\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64 * block.expansion, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet32(num_classes=100):\n",
    "    \"\"\"ResNet-32 for CIFAR-100.\"\"\"\n",
    "    return ResNet(BasicBlock, [5, 5, 5], num_classes=num_classes)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Utilities\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_agar_confidence(optimizer):\n",
    "    \"\"\"Extract current AGAR and confidence from CASMO optimizer.\"\"\"\n",
    "    if not hasattr(optimizer, '_group_states'):\n",
    "        return None, None\n",
    "    group_state = optimizer._group_states.get(0, {})\n",
    "    return group_state.get('current_agar'), group_state.get('current_confidence')\n",
    "\n",
    "\n",
    "def compute_per_group_accuracy(model, loader, many_shot_classes, medium_shot_classes, few_shot_classes, device):\n",
    "    \"\"\"Compute accuracy for many/medium/few shot classes.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    many_correct = 0\n",
    "    many_total = 0\n",
    "    medium_correct = 0\n",
    "    medium_total = 0\n",
    "    few_correct = 0\n",
    "    few_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            for pred, target in zip(predicted, targets):\n",
    "                target_item = target.item()\n",
    "                correct = (pred == target).item()\n",
    "                \n",
    "                if target_item in many_shot_classes:\n",
    "                    many_total += 1\n",
    "                    many_correct += correct\n",
    "                elif target_item in medium_shot_classes:\n",
    "                    medium_total += 1\n",
    "                    medium_correct += correct\n",
    "                elif target_item in few_shot_classes:\n",
    "                    few_total += 1\n",
    "                    few_correct += correct\n",
    "    \n",
    "    many_acc = 100. * many_correct / many_total if many_total > 0 else 0\n",
    "    medium_acc = 100. * medium_correct / medium_total if medium_total > 0 else 0\n",
    "    few_acc = 100. * few_correct / few_total if few_total > 0 else 0\n",
    "    \n",
    "    return many_acc, medium_acc, few_acc\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Training Loop\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def run_benchmark(optimizer_name, device, train_loader, test_loader, train_dataset, \n",
    "                 num_epochs=200, lr=0.1, weight_decay=5e-4, seed=42):\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running: {optimizer_name.upper()}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    model = ResNet32(num_classes=100)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if optimizer_name == 'casmo':\n",
    "        total_steps = len(train_loader) * num_epochs\n",
    "        tau_init_steps = max(50, int(0.05 * total_steps))\n",
    "        \n",
    "        optimizer = CASMO(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            granularity='group',\n",
    "            log_level=1,\n",
    "            tau_init_steps=tau_init_steps,\n",
    "            tau_dead_zone=0.2,\n",
    "            c_min=0.1\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Cosine annealing scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Get class groups for per-group accuracy\n",
    "    many_shot, medium_shot, few_shot = train_dataset.get_class_groups()\n",
    "    \n",
    "    results = {\n",
    "        'train_accs': [],\n",
    "        'val_accs': [],\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'many_shot_accs': [],\n",
    "        'medium_shot_accs': [],\n",
    "        'few_shot_accs': [],\n",
    "        'agar_values': [],\n",
    "        'confidence_values': [],\n",
    "        'steps': []\n",
    "    }\n",
    "    \n",
    "    step = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Log AGAR and confidence\n",
    "            agar, confidence = get_agar_confidence(optimizer)\n",
    "            if agar is not None and step % 10 == 0:\n",
    "                results['agar_values'].append(agar)\n",
    "                results['confidence_values'].append(confidence)\n",
    "                results['steps'].append(step)\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        val_loss /= len(test_loader)\n",
    "        \n",
    "        # Compute per-group accuracy\n",
    "        many_acc, medium_acc, few_acc = compute_per_group_accuracy(\n",
    "            model, test_loader, many_shot, medium_shot, few_shot, device\n",
    "        )\n",
    "        \n",
    "        results['train_accs'].append(train_acc)\n",
    "        results['val_accs'].append(val_acc)\n",
    "        results['train_losses'].append(train_loss)\n",
    "        results['val_losses'].append(val_loss)\n",
    "        results['many_shot_accs'].append(many_acc)\n",
    "        results['medium_shot_accs'].append(medium_acc)\n",
    "        results['few_shot_accs'].append(few_acc)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}% | \"\n",
    "                  f\"Many: {many_acc:.1f}% | Medium: {medium_acc:.1f}% | Few: {few_acc:.1f}%\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Finished in {total_time:.1f}s\")\n",
    "    \n",
    "    # Print final AGAR/Confidence statistics\n",
    "    if optimizer_name == 'casmo' and results['agar_values']:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Final AGAR Statistics (last 100 steps):\")\n",
    "        print(f\"{'='*70}\")\n",
    "        agar_final = results['agar_values'][-100:] if len(results['agar_values']) >= 100 else results['agar_values']\n",
    "        print(f\"  Mean: {np.mean(agar_final):.4f}\")\n",
    "        print(f\"  Std:  {np.std(agar_final):.4f}\")\n",
    "        print(f\"  Min:  {min(agar_final):.4f}\")\n",
    "        print(f\"  Max:  {max(agar_final):.4f}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Final Confidence Statistics (last 100 steps):\")\n",
    "        print(f\"{'='*70}\")\n",
    "        conf_final = results['confidence_values'][-100:] if len(results['confidence_values']) >= 100 else results['confidence_values']\n",
    "        print(f\"  Mean: {np.mean(conf_final):.4f}\")\n",
    "        print(f\"  Std:  {np.std(conf_final):.4f}\")\n",
    "        print(f\"  Min:  {min(conf_final):.4f}\")\n",
    "        print(f\"  Max:  {max(conf_final):.4f}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Main Execution\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"CASMO vs AdamW Benchmark - Long-Tailed CIFAR-100\")\n",
    "    print(f\"Imbalance Factor: {100}:1\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Setup Data\n",
    "    train_dataset = LongTailCIFAR100(root='./data', train=True, imbalance_factor=100)\n",
    "    test_dataset = LongTailCIFAR100(root='./data', train=False, imbalance_factor=100)\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    many_shot, medium_shot, few_shot = train_dataset.get_class_groups()\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total training samples: {len(train_dataset)}\")\n",
    "    print(f\"  Many-shot classes (>100 samples): {len(many_shot)}\")\n",
    "    print(f\"  Medium-shot classes (20-100 samples): {len(medium_shot)}\")\n",
    "    print(f\"  Few-shot classes (<20 samples): {len(few_shot)}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Run Benchmarks\n",
    "    print(\"\\nStarting CASMO run...\")\n",
    "    casmo_results = run_benchmark('casmo', device, train_loader, test_loader, train_dataset,\n",
    "                                  num_epochs=200, lr=0.1, weight_decay=5e-4)\n",
    "    \n",
    "    print(\"\\nStarting AdamW run...\")\n",
    "    adamw_results = run_benchmark('adamw', device, train_loader, test_loader, train_dataset,\n",
    "                                  num_epochs=200, lr=0.1, weight_decay=5e-4)\n",
    "    \n",
    "    # Plotting\n",
    "    print(\"\\nPlotting results...\")\n",
    "    \n",
    "    # Create results directory if it doesn't exist\n",
    "    results_dir = \"results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Overall Accuracy Curves\n",
    "    axes[0, 0].plot(casmo_results['train_accs'], label='CASMO Train', linestyle='--', alpha=0.5, color='green')\n",
    "    axes[0, 0].plot(casmo_results['val_accs'], label='CASMO Val', linewidth=2, color='green')\n",
    "    axes[0, 0].plot(adamw_results['train_accs'], label='AdamW Train', linestyle='--', alpha=0.5, color='orange')\n",
    "    axes[0, 0].plot(adamw_results['val_accs'], label='AdamW Val', linewidth=2, color='orange')\n",
    "    axes[0, 0].set_title(f'Overall Accuracy (Imbalance: {args.imbalance}:1)')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Per-Group Accuracy\n",
    "    epochs = range(len(casmo_results['many_shot_accs']))\n",
    "    axes[0, 1].plot(epochs, casmo_results['many_shot_accs'], label='CASMO Many-shot', color='green', linestyle='-', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, casmo_results['medium_shot_accs'], label='CASMO Medium-shot', color='green', linestyle='--', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, casmo_results['few_shot_accs'], label='CASMO Few-shot', color='green', linestyle=':', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, adamw_results['many_shot_accs'], label='AdamW Many-shot', color='orange', linestyle='-', linewidth=2, alpha=0.7)\n",
    "    axes[0, 1].plot(epochs, adamw_results['medium_shot_accs'], label='AdamW Medium-shot', color='orange', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    axes[0, 1].plot(epochs, adamw_results['few_shot_accs'], label='AdamW Few-shot', color='orange', linestyle=':', linewidth=2, alpha=0.7)\n",
    "    axes[0, 1].set_title('Per-Group Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend(fontsize=8)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. AGAR Evolution (CASMO only)\n",
    "    if casmo_results['agar_values']:\n",
    "        axes[1, 0].plot(casmo_results['steps'], casmo_results['agar_values'], color='green', alpha=0.6)\n",
    "        axes[1, 0].set_title('CASMO: AGAR Signal Strength')\n",
    "        axes[1, 0].set_xlabel('Step')\n",
    "        axes[1, 0].set_ylabel('AGAR')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Confidence Evolution (CASMO only)\n",
    "    if casmo_results.get('confidence_values'):\n",
    "        axes[1, 1].plot(casmo_results['steps'], casmo_results['confidence_values'], color='blue', alpha=0.6)\n",
    "        axes[1, 1].set_title('CASMO: Confidence Score')\n",
    "        axes[1, 1].set_xlabel('Step')\n",
    "        axes[1, 1].set_ylabel('Confidence')\n",
    "        axes[1, 1].set_ylim(0, 1.05)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, 'long_tail_metrics.png')\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    print(f\"✅ Plot saved to {save_path}\")\n",
    "    \n",
    "    # Print final comparison\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FINAL RESULTS COMPARISON\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Metric':<30} {'CASMO':>15} {'AdamW':>15}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"{'Overall Val Accuracy':<30} {casmo_results['val_accs'][-1]:>14.2f}% {adamw_results['val_accs'][-1]:>14.2f}%\")\n",
    "    print(f\"{'Many-shot Accuracy':<30} {casmo_results['many_shot_accs'][-1]:>14.2f}% {adamw_results['many_shot_accs'][-1]:>14.2f}%\")\n",
    "    print(f\"{'Medium-shot Accuracy':<30} {casmo_results['medium_shot_accs'][-1]:>14.2f}% {adamw_results['medium_shot_accs'][-1]:>14.2f}%\")\n",
    "    print(f\"{'Few-shot Accuracy':<30} {casmo_results['few_shot_accs'][-1]:>14.2f}% {adamw_results['few_shot_accs'][-1]:>14.2f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
