{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d281131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.2\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39598cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NoisyInstructDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Noisy Instruction Following.\n",
    "    Loads UltraFeedback and injects noise by flipping 'chosen' and 'rejected' responses.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, split='train', max_length=512, noise_ratio=0.3, seed=42):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.noise_ratio = noise_ratio\n",
    "        \n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Load UltraFeedback (binarized preferences)\n",
    "        # Using a smaller subset or a compatible version if needed. \n",
    "        # 'HuggingFaceH4/ultrafeedback_binarized' is a good pre-processed version.\n",
    "        print(f\"Loading dataset: HuggingFaceH4/ultrafeedback_binarized [{split}]...\")\n",
    "        try:\n",
    "            self.dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=f\"{split}_sft[:10000]\") # Limit to 10k for speed as per plan\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}. Fallback to local mock if needed (not implemented).\")\n",
    "            raise e\n",
    "            \n",
    "        self.data = []\n",
    "        self.process_data()\n",
    "\n",
    "    def process_data(self):\n",
    "        print(f\"Processing {len(self.dataset)} samples with {self.noise_ratio*100}% noise...\")\n",
    "        \n",
    "        for i, item in enumerate(self.dataset):\n",
    "            # Format: messages usually contain list of dicts [{'role': 'user', ...}, {'role': 'assistant', ...}]\n",
    "            # In binarized version, we often have 'chosen' and 'rejected' lists of messages.\n",
    "            \n",
    "            prompt = item['prompt']\n",
    "            chosen = item['chosen'][-1]['content'] # Last message is assistant response\n",
    "            rejected = item['rejected'][-1]['content']\n",
    "            \n",
    "            is_noisy = False\n",
    "            \n",
    "            # Noise Injection\n",
    "            if random.random() < self.noise_ratio:\n",
    "                is_noisy = True\n",
    "                # Flip: The \"target\" becomes the bad response\n",
    "                target_response = rejected\n",
    "            else:\n",
    "                target_response = chosen\n",
    "                \n",
    "            # Construct input text for Causal LM\n",
    "            # Format: \"### Instruction: ... ### Response: ...\" or ChatML\n",
    "            # Phi-2 is flexible, let's use a simple format.\n",
    "            text = f\"Instruct: {prompt}\\nOutput: {target_response}\"\n",
    "            \n",
    "            self.data.append({\n",
    "                'text': text,\n",
    "                'is_noisy': is_noisy\n",
    "            })\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Labels are same as input (standard Causal LM training)\n",
    "        # We could mask the instruction part, but for simple benchmark, full sequence loss is fine.\n",
    "        input_ids = encodings['input_ids'].squeeze()\n",
    "        attention_mask = encodings['attention_mask'].squeeze()\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'is_noisy': torch.tensor(item['is_noisy'], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d89db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CASMO: Confident Adaptive Selective Momentum Optimizer\n",
    "\n",
    "A production-ready PyTorch optimizer that extends Adam with confidence-based learning rate scaling.\n",
    "\n",
    "Core Innovation: AGAR (Adaptive Gradient Alignment Ratio)\n",
    "    AGAR = ||E[g]||² / (||E[g]||² + Var[g])\n",
    "    \n",
    "    Measures signal (consistent gradient direction) vs noise (random fluctuations).\n",
    "    Naturally ranges from 0 (pure noise) to 1 (pure signal) for interpretable confidence metrics.\n",
    "\n",
    "Performance:\n",
    "    - Faster than AdamW on large models (-2% overhead with per-group mode)\n",
    "    - Configurable granularity for speed/precision tradeoff\n",
    "    - Pre-allocated buffers eliminate allocation overhead\n",
    "\n",
    "Usage Example:\n",
    "    >>> from casmo import CASMO\n",
    "    >>> optimizer = CASMO(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    >>> for epoch in range(num_epochs):\n",
    "    ...     for batch in dataloader:\n",
    "    ...         loss = model(batch)\n",
    "    ...         loss.backward()\n",
    "    ...         optimizer.step()\n",
    "    ...         optimizer.zero_grad()\n",
    "\n",
    "Reference: \n",
    "    Kingma & Ba (2015). \"Adam: A Method for Stochastic Optimization\"\n",
    "    https://arxiv.org/abs/1412.6980\n",
    "\"\"\"\n",
    "\n",
    "from typing import Tuple, Optional, Callable, Dict, Any\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import logging\n",
    "\n",
    "\n",
    "class DDEAdapter:\n",
    "    \"\"\"\n",
    "    Drift-Detecting EMA adapter for tau threshold adjustment.\n",
    "    \n",
    "    Tracks AGAR variance to adaptively adjust tau while preventing\n",
    "    runaway adaptation to noise or memorization signals.\n",
    "    O(1) memory and compute per step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # EMA update rates\n",
    "    EMA_MEAN_RATE = 0.001\n",
    "    EMA_VAR_DECAY = 0.99\n",
    "    EMA_VAR_RATE = 0.01\n",
    "    \n",
    "    # Adaptive gain bounds\n",
    "    MIN_GAIN = 0.001\n",
    "    MAX_GAIN = 0.01\n",
    "    GAIN_SCALE = 0.1\n",
    "    \n",
    "    # Memorization detection threshold\n",
    "    MEMORIZATION_FACTOR = 1.2\n",
    "    \n",
    "    def __init__(self, tau_init: float, tau_clip_range: Tuple[float, float], \n",
    "                 dead_zone_factor: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the DDE adapter.\n",
    "        \n",
    "        Args:\n",
    "            tau_init: Initial tau value\n",
    "            tau_clip_range: (min, max) bounds for tau\n",
    "            dead_zone_factor: Ignore deviations smaller than this fraction of tau.\n",
    "                Prevents chasing noise. Default: 0.2 (20%)\n",
    "        \"\"\"\n",
    "        self.tau = tau_init\n",
    "        self.tau_calibrated: Optional[float] = None\n",
    "        self.clip_range = tau_clip_range\n",
    "        self.dead_zone = dead_zone_factor\n",
    "        \n",
    "        # EMA state for variance tracking\n",
    "        self.mean_agar = tau_init\n",
    "        self.ema_var = 0.01\n",
    "    \n",
    "    def update(self, agar_value: float) -> float:\n",
    "        \"\"\"\n",
    "        Update tau threshold using variance-adaptive gain and dead zone filtering.\n",
    "        \n",
    "        Args:\n",
    "            agar_value: Current AGAR measurement\n",
    "            \n",
    "        Returns:\n",
    "            Updated tau value (clipped to valid range)\n",
    "        \"\"\"\n",
    "        # Update EMA mean\n",
    "        diff = agar_value - self.mean_agar\n",
    "        self.mean_agar += self.EMA_MEAN_RATE * diff\n",
    "        \n",
    "        # Update EMA variance: Var[X] = E[(X - μ)²]\n",
    "        self.ema_var = self.EMA_VAR_DECAY * self.ema_var + self.EMA_VAR_RATE * (diff ** 2)\n",
    "        \n",
    "        # Relative variance (scale-invariant)\n",
    "        rel_var = self.ema_var / (self.mean_agar + 1e-8)\n",
    "        \n",
    "        # Prevent tau from chasing memorization signals\n",
    "        if self.tau_calibrated is not None and agar_value > self.MEMORIZATION_FACTOR * self.tau_calibrated:\n",
    "            # AGAR suspiciously high - likely overfitting, freeze tau\n",
    "            return self.tau\n",
    "        \n",
    "        # Dead zone: only adapt if deviation exceeds threshold\n",
    "        dead_zone_reference = self.tau_calibrated if self.tau_calibrated is not None else self.tau\n",
    "        deviation = abs(agar_value - self.tau)\n",
    "        if deviation > self.dead_zone * dead_zone_reference:\n",
    "            # Variance-adaptive gain: higher variance → faster adaptation\n",
    "            alpha = self.MIN_GAIN + min(rel_var * self.GAIN_SCALE, self.MAX_GAIN - self.MIN_GAIN)\n",
    "            new_tau = (1 - alpha) * self.tau + alpha * agar_value\n",
    "            \n",
    "            # Never decrease tau below calibrated baseline\n",
    "            if self.tau_calibrated is not None:\n",
    "                new_tau = max(new_tau, self.tau_calibrated)\n",
    "            \n",
    "            self.tau = new_tau\n",
    "        \n",
    "        return float(np.clip(self.tau, self.clip_range[0], self.clip_range[1]))\n",
    "\n",
    "\n",
    "class CASMO(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Confident Adaptive Selective Momentum Optimizer.\n",
    "    \n",
    "    Extends Adam with confidence-based learning rate scaling using AGAR metrics.\n",
    "    Automatically adapts to gradient signal-to-noise ratio for improved convergence.\n",
    "    \n",
    "    Uses universal sigmoid-based confidence mapping that adapts to any noise distribution:\n",
    "    - Clean data: High confidence baseline\n",
    "    - Pervasive noise: Adaptive scaling with high c_min\n",
    "    - Mixed batches: Strong discrimination via distribution statistics\n",
    "    \n",
    "    Args:\n",
    "        params (iterable): Iterable of parameters to optimize or dicts defining parameter groups\n",
    "        lr (float, optional): Learning rate. Default: 1e-3\n",
    "        betas (Tuple[float, float], optional): Coefficients for computing running averages \n",
    "            of gradient and its square (β₁, β₂). Default: (0.9, 0.999)\n",
    "        eps (float, optional): Term added to denominator for numerical stability. Default: 1e-8\n",
    "        weight_decay (float, optional): Decoupled weight decay coefficient (AdamW-style). \n",
    "            Default: 0.0\n",
    "        tau_init_steps (int, optional): Number of initial steps to collect AGAR samples \n",
    "            for automatic threshold calibration. Must be >= 50. Default: 500\n",
    "            Recommended: max(500, int(0.05 * total_steps))\n",
    "        tau_clip_range (Tuple[float, float], optional): Min/max bounds for tau threshold. \n",
    "            Default: (0.01, 0.5)\n",
    "        tau_dead_zone (float, optional): Dead zone factor for tau adaptation.\n",
    "            Ignores AGAR deviations smaller than this fraction of tau to prevent chasing noise.\n",
    "            Default: 0.2 (20%)\n",
    "        c_min (float, optional): Minimum confidence scaling factor to prevent learning rate \n",
    "            from becoming too small. Must be in [0, 1]. Default: 0.1\n",
    "            Note: After calibration, c_min is automatically computed based on noise level.\n",
    "        granularity (str, optional): AGAR computation granularity.\n",
    "            - 'parameter': Per-parameter confidence scaling (~13% overhead on large models).\n",
    "              Use for small models (<10M params) or when layer-specific adaptation matters.\n",
    "            - 'group': Per-group confidence scaling (faster than AdamW on large models).\n",
    "              Recommended for production use, large models (>10M params), and hyperparameter sweeps.\n",
    "            Default: 'group'\n",
    "        agar_clamp_factor (float, optional): Outlier clamping factor for AGAR computation.\n",
    "            Clamps moment estimates to ±(mean * factor) to handle extreme values.\n",
    "            Set to None to disable clamping. Default: 10.0\n",
    "        log_level (int, optional): Logging verbosity. 0=silent, 1=errors, 2=warnings, \n",
    "            3=info. Default: 1\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any parameter is outside its valid range\n",
    "        RuntimeError: If NaN or Inf gradients are detected during optimization\n",
    "        NotImplementedError: If sparse gradients are encountered\n",
    "    \n",
    "    Note:\n",
    "        This optimizer does not support sparse gradients. Use torch.optim.SparseAdam\n",
    "        for sparse gradient scenarios.\n",
    "    \n",
    "    Example:\n",
    "        >>> model = YourModel()\n",
    "        >>> optimizer = CASMO(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "        >>> \n",
    "        >>> for epoch in range(num_epochs):\n",
    "        ...     for batch in dataloader:\n",
    "        ...         optimizer.zero_grad()\n",
    "        ...         loss = model(batch)\n",
    "        ...         loss.backward()\n",
    "        ...         optimizer.step()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.0,\n",
    "        tau_init_steps: int = 500,\n",
    "        tau_clip_range: Tuple[float, float] = (0.01, 0.5),\n",
    "        tau_dead_zone: float = 0.2,  # Large dead zone to prevent chasing memorization\n",
    "        c_min: float = 0.1,\n",
    "        granularity: str = 'group',\n",
    "        agar_clamp_factor: Optional[float] = 10.0,\n",
    "        log_level: int = 1,\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta1: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta2: {betas[1]}\")\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(f\"Invalid weight_decay: {weight_decay}\")\n",
    "        if not 0.0 <= c_min <= 1.0:\n",
    "            raise ValueError(f\"Invalid c_min: {c_min}\")\n",
    "        if tau_init_steps < 50:\n",
    "            raise ValueError(f\"tau_init_steps too small: {tau_init_steps} (minimum: 50)\")\n",
    "        if not 0.0 <= tau_dead_zone <= 1.0:\n",
    "            raise ValueError(f\"Invalid tau_dead_zone: {tau_dead_zone} (must be in [0, 1])\")\n",
    "        if granularity not in ['parameter', 'group']:\n",
    "            raise ValueError(f\"Invalid granularity: {granularity} (must be 'parameter' or 'group')\")\n",
    "        \n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            tau_init_steps=tau_init_steps,\n",
    "            tau_clip_range=tau_clip_range,\n",
    "            tau_dead_zone=tau_dead_zone,\n",
    "            c_min=c_min,\n",
    "            granularity=granularity,\n",
    "            agar_clamp_factor=agar_clamp_factor,\n",
    "        )\n",
    "        \n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger('CASMO')\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(logging.Formatter('[CASMO] %(message)s'))\n",
    "            self.logger.addHandler(handler)\n",
    "        self.logger.setLevel(self._get_log_level(log_level))\n",
    "        \n",
    "        self._step_count = 0\n",
    "        \n",
    "        # Initialize per-group state for tau calibration and buffer reuse\n",
    "        self._group_states: Dict[int, Dict[str, Any]] = {}\n",
    "        for idx, group in enumerate(self.param_groups):\n",
    "            group_tau_dead_zone = group.get('tau_dead_zone', tau_dead_zone)\n",
    "            group_tau_clip_range = group.get('tau_clip_range', tau_clip_range)\n",
    "            group_tau_init_steps = group.get('tau_init_steps', tau_init_steps)\n",
    "            \n",
    "            self._group_states[idx] = {\n",
    "                'tau_adapter': DDEAdapter(1.0, group_tau_clip_range, dead_zone_factor=group_tau_dead_zone),\n",
    "                'tau_initialized': False,\n",
    "                'agar_buffer': deque(maxlen=group_tau_init_steps),\n",
    "                'reuse_buffer_exp_avg': None,\n",
    "                'reuse_buffer_exp_avg_sq': None,\n",
    "                'current_agar': None,\n",
    "                'current_confidence': None,\n",
    "                'agar_mean': None,\n",
    "                'agar_std': None,\n",
    "                'agar_median': None,\n",
    "                'agar_p10': None,\n",
    "                'agar_p90': None,\n",
    "                'c_min': c_min,\n",
    "            }\n",
    "    \n",
    "    def _get_log_level(self, level: int) -> int:\n",
    "        \"\"\"\n",
    "        Convert custom log level to Python logging level.\n",
    "        \n",
    "        Args:\n",
    "            level: Custom level (0=silent, 1=error, 2=warning, 3=info)\n",
    "            \n",
    "        Returns:\n",
    "            Python logging level constant\n",
    "        \"\"\"\n",
    "        level_map = {\n",
    "            0: logging.CRITICAL + 1,  # Silent\n",
    "            1: logging.ERROR,\n",
    "            2: logging.WARNING,\n",
    "            3: logging.INFO,\n",
    "        }\n",
    "        return level_map.get(level, logging.WARNING)\n",
    "    \n",
    "    def _log(self, level: int, message: str) -> None:\n",
    "        \"\"\"\n",
    "        Internal logging utility using Python logging module.\n",
    "        \n",
    "        Args:\n",
    "            level: Message severity level (1=error, 2=warning, 3=info)\n",
    "            message: Log message to output\n",
    "        \"\"\"\n",
    "        if level == 1:\n",
    "            self.logger.error(message)\n",
    "        elif level == 2:\n",
    "            self.logger.warning(message)\n",
    "        elif level == 3:\n",
    "            self.logger.info(message)\n",
    "    \n",
    "    def _validate_gradient(self, grad: torch.Tensor, group_idx: int) -> None:\n",
    "        \"\"\"\n",
    "        Validate gradient for NaN, Inf, and sparse tensors.\n",
    "        \n",
    "        Args:\n",
    "            grad: Gradient tensor to validate\n",
    "            group_idx: Parameter group index for error messages\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: If NaN or Inf detected\n",
    "            NotImplementedError: If sparse gradient detected\n",
    "        \"\"\"\n",
    "        if torch.isnan(grad).any():\n",
    "            raise RuntimeError(\n",
    "                f\"NaN gradient detected in parameter group {group_idx}. \"\n",
    "                \"Consider using gradient clipping: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\"\n",
    "            )\n",
    "        if torch.isinf(grad).any():\n",
    "            raise RuntimeError(\n",
    "                f\"Inf gradient detected in parameter group {group_idx}. \"\n",
    "                \"Check for numerical overflow in loss computation or model outputs.\"\n",
    "            )\n",
    "        if grad.is_sparse:\n",
    "            raise NotImplementedError(\n",
    "                \"CASMO does not support sparse gradients. \"\n",
    "                \"Use torch.optim.SparseAdam for sparse gradient scenarios, \"\n",
    "                \"or convert gradients to dense format with grad.to_dense().\"\n",
    "            )\n",
    "    \n",
    "    def _init_param_state(self, p: torch.Tensor) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Initialize optimizer state for a parameter.\n",
    "        \n",
    "        Args:\n",
    "            p: Parameter tensor\n",
    "            \n",
    "        Returns:\n",
    "            Initialized state dictionary with step counter and moment estimates\n",
    "        \"\"\"\n",
    "        state = self.state[p]\n",
    "        if len(state) == 0:\n",
    "            state['step'] = 0\n",
    "            state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "            state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "    \n",
    "    def _update_moments(self, state: Dict[str, Any], grad: torch.Tensor, beta1: float, beta2: float) -> None:\n",
    "        \"\"\"\n",
    "        Update exponential moving averages of gradient moments.\n",
    "        \n",
    "        Args:\n",
    "            state: Parameter state dictionary\n",
    "            grad: Current gradient\n",
    "            beta1: First moment decay rate (β₁)\n",
    "            beta2: Second moment decay rate (β₂)\n",
    "        \"\"\"\n",
    "        exp_avg = state['exp_avg']\n",
    "        exp_avg_sq = state['exp_avg_sq']\n",
    "        state['step'] += 1\n",
    "        \n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "    \n",
    "    def _apply_weight_update(self, p: torch.Tensor, state: Dict[str, Any], lr: float, \n",
    "                            weight_decay: float, eps: float, confidence: torch.Tensor,\n",
    "                            beta1: float, beta2: float) -> None:\n",
    "        \"\"\"\n",
    "        Apply Adam-style parameter update with confidence-scaled learning rate.\n",
    "        \n",
    "        Implements decoupled weight decay (AdamW) with bias-corrected moments\n",
    "        and confidence-based learning rate modulation.\n",
    "        \n",
    "        Args:\n",
    "            p: Parameter tensor to update\n",
    "            state: Parameter state dictionary containing moments\n",
    "            lr: Base learning rate\n",
    "            weight_decay: Decoupled weight decay coefficient\n",
    "            eps: Numerical stability constant (ε)\n",
    "            confidence: Confidence scaling factor in [c_min, 1.0]\n",
    "            beta1: First moment decay rate (β₁)\n",
    "            beta2: Second moment decay rate (β₂)\n",
    "        \"\"\"\n",
    "        exp_avg = state['exp_avg']\n",
    "        exp_avg_sq = state['exp_avg_sq']\n",
    "        step = state['step']\n",
    "        \n",
    "        # Bias correction\n",
    "        bias_correction1 = 1 - beta1 ** step\n",
    "        bias_correction2 = 1 - beta2 ** step\n",
    "        m_hat = exp_avg / bias_correction1\n",
    "        v_hat = exp_avg_sq / bias_correction2\n",
    "        \n",
    "        # Weight decay (decoupled)\n",
    "        if weight_decay != 0:\n",
    "            p.mul_(1 - lr * weight_decay)\n",
    "        \n",
    "        # Apply update with confidence-scaled learning rate\n",
    "        denom = v_hat.sqrt().add_(eps)\n",
    "        step_size = lr * confidence\n",
    "        p.addcdiv_(m_hat, denom, value=-step_size)\n",
    "    \n",
    "    def _compute_agar(\n",
    "        self,\n",
    "        exp_avg: torch.Tensor,\n",
    "        exp_avg_sq: torch.Tensor,\n",
    "        eps: float,\n",
    "        clamp_factor: Optional[float],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Adaptive Gradient Alignment Ratio (AGAR) from exponential moving averages.\n",
    "        \n",
    "        AGAR quantifies the signal-to-noise ratio in gradients:\n",
    "            AGAR = mean(signal / (signal + noise))\n",
    "            where signal = (E[g])² (squared mean gradient per element)\n",
    "                  noise = Var[g] = E[g²] - (E[g])² (gradient variance per element)\n",
    "        \n",
    "        Args:\n",
    "            exp_avg (torch.Tensor): Exponential moving average of gradients (first moment)\n",
    "            exp_avg_sq (torch.Tensor): Exponential moving average of squared gradients (second moment)\n",
    "            eps (float): Small constant for numerical stability\n",
    "            clamp_factor (Optional[float]): Outlier clamping factor (None to disable)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Scalar AGAR value in range [0, 1], where:\n",
    "                - 0 indicates pure noise (no consistent gradient direction)\n",
    "                - 1 indicates pure signal (perfectly consistent gradients)\n",
    "        \n",
    "        Note:\n",
    "            AGAR is computed per-element then uniformly averaged across all elements.\n",
    "            This provides robustness across parameters with different scales.\n",
    "            Uses raw moments to preserve the variance relationship Var[g] = E[g²] - (E[g])².\n",
    "            Bias correction would distort this relationship and cause AGAR instability.\n",
    "        \"\"\"\n",
    "        # Outlier protection: clamp extreme values based on gradient statistics\n",
    "        if clamp_factor is not None:\n",
    "            m_scale = exp_avg.abs().mean() + eps\n",
    "            v_scale = exp_avg_sq.mean() + eps\n",
    "            m_clamped = torch.clamp(exp_avg, min=-m_scale * clamp_factor, max=m_scale * clamp_factor)\n",
    "            v_clamped = torch.clamp(exp_avg_sq, min=0.0, max=v_scale * clamp_factor)\n",
    "        else:\n",
    "            m_clamped = exp_avg\n",
    "            v_clamped = exp_avg_sq\n",
    "        \n",
    "        # Signal: squared norm of mean gradient (consistent direction)\n",
    "        signal_per_elem = m_clamped.pow(2)\n",
    "        \n",
    "        # Noise: gradient variance = E[g²] - (E[g])²\n",
    "        noise_per_elem = torch.clamp(v_clamped - signal_per_elem, min=eps)\n",
    "        \n",
    "        # Compute mean AGAR across all elements (uniform weighting)\n",
    "        agar_per_elem = signal_per_elem / (signal_per_elem + noise_per_elem + eps)\n",
    "        agar = agar_per_elem.mean()\n",
    "        \n",
    "        return torch.clamp(agar, min=0.0, max=1.0)\n",
    "    \n",
    "    # Calibration constants\n",
    "    MIN_CALIBRATION_SAMPLES = 50\n",
    "    MIN_STD_THRESHOLD = 0.01  # Prevent division by zero\n",
    "    \n",
    "    # Coefficient of variation thresholds for adaptive c_min\n",
    "    CV_HIGH_THRESHOLD = 0.5  # Bimodal distribution\n",
    "    CV_MEDIUM_THRESHOLD = 0.3  # Some separation\n",
    "    \n",
    "    # Adaptive c_min values\n",
    "    C_MIN_HIGH_VARIANCE = 0.1  # Strong discrimination for bimodal\n",
    "    C_MIN_MEDIUM_VARIANCE = 0.3  # Moderate discrimination\n",
    "    C_MIN_LOW_VARIANCE = 0.5  # High baseline for unimodal/pervasive noise\n",
    "    \n",
    "    def _calibrate_tau(self, agar_buffer: deque, tau_clip_range: Tuple[float, float], group_idx: int) -> float:\n",
    "        \"\"\"\n",
    "        Universal tau calibration using distribution statistics.\n",
    "        \n",
    "        Computes distribution parameters for confidence mapping:\n",
    "        - μ (mean): Central tendency of AGAR distribution\n",
    "        - σ (std): Spread of AGAR distribution\n",
    "        - p50 (median): Robust center estimate\n",
    "        - p10, p90: Distribution bounds for outlier detection\n",
    "        \n",
    "        This approach works universally for:\n",
    "        - Clean data: High μ, low σ → High confidence baseline\n",
    "        - Pervasive noise: Low μ, low σ → Adaptive confidence scaling\n",
    "        - Mixed batches: Medium μ, high σ → Bimodal confidence distribution\n",
    "        \n",
    "        Mathematical foundation:\n",
    "            confidence(agar) = c_min + (1 - c_min) * sigmoid((agar - μ) / σ)\n",
    "        \n",
    "        This sigmoid mapping naturally adapts to any distribution shape.\n",
    "        \n",
    "        Args:\n",
    "            agar_buffer: Collection of AGAR samples from initial training steps\n",
    "            tau_clip_range: Safety bounds for tau (min, max)\n",
    "            group_idx: Parameter group index for storing calibration results\n",
    "        \n",
    "        Returns:\n",
    "            Calibrated tau threshold (median for robustness)\n",
    "        \"\"\"\n",
    "        if len(agar_buffer) < self.MIN_CALIBRATION_SAMPLES:\n",
    "            return tau_clip_range[1]\n",
    "        \n",
    "        samples = np.array(agar_buffer)\n",
    "        \n",
    "        # Distribution statistics\n",
    "        mu = np.mean(samples)\n",
    "        sigma = np.std(samples)\n",
    "        median = np.median(samples)\n",
    "        p10 = np.percentile(samples, 10)\n",
    "        p90 = np.percentile(samples, 90)\n",
    "        \n",
    "        # Store distribution parameters for confidence mapping\n",
    "        group_state = self._group_states[group_idx]\n",
    "        group_state['agar_mean'] = float(mu)\n",
    "        group_state['agar_std'] = float(max(sigma, self.MIN_STD_THRESHOLD))\n",
    "        group_state['agar_median'] = float(median)\n",
    "        group_state['agar_p10'] = float(p10)\n",
    "        group_state['agar_p90'] = float(p90)\n",
    "        \n",
    "        # Adaptive c_min based on coefficient of variation (CV = σ/μ)\n",
    "        # High CV → Lower c_min (strong discrimination for bimodal distributions)\n",
    "        # Low CV → Higher c_min (prevent over-suppression for unimodal/pervasive noise)\n",
    "        cv = sigma / (mu + 1e-8)\n",
    "        if cv > self.CV_HIGH_THRESHOLD:\n",
    "            c_min_adaptive = self.C_MIN_HIGH_VARIANCE\n",
    "        elif cv > self.CV_MEDIUM_THRESHOLD:\n",
    "            c_min_adaptive = self.C_MIN_MEDIUM_VARIANCE\n",
    "        else:\n",
    "            c_min_adaptive = self.C_MIN_LOW_VARIANCE\n",
    "        \n",
    "        group_state['c_min'] = float(c_min_adaptive)\n",
    "        \n",
    "        self._log(2, f\"Calibrated AGAR distribution: μ={mu:.4f}, σ={sigma:.4f}, \"\n",
    "                     f\"median={median:.4f}, CV={cv:.4f}, c_min={c_min_adaptive:.2f}\")\n",
    "        \n",
    "        # Return median as tau (robust to outliers)\n",
    "        return float(np.clip(median, tau_clip_range[0], tau_clip_range[1]))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Optional[Callable] = None) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Perform a single optimization step.\n",
    "        \n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model and returns\n",
    "                the loss. Optional for most optimizers but required for some (e.g., LBFGS).\n",
    "        \n",
    "        Returns:\n",
    "            Optional[float]: Loss value if closure is provided, None otherwise\n",
    "        \n",
    "        Raises:\n",
    "            RuntimeError: If NaN or Inf gradients are detected\n",
    "            NotImplementedError: If sparse gradients are encountered\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        \n",
    "        self._step_count += 1\n",
    "        \n",
    "        for group_idx, group in enumerate(self.param_groups):\n",
    "            beta1, beta2 = group['betas']\n",
    "            lr = group['lr']\n",
    "            eps = group['eps']\n",
    "            weight_decay = group['weight_decay']\n",
    "            c_min = group['c_min']\n",
    "            tau_init_steps = group['tau_init_steps']\n",
    "            tau_clip_range = group['tau_clip_range']\n",
    "            granularity = group['granularity']\n",
    "            agar_clamp_factor = group['agar_clamp_factor']\n",
    "            \n",
    "            group_state = self._group_states[group_idx]\n",
    "            \n",
    "            # Per-group AGAR mode: compute once for all parameters in group\n",
    "            if granularity == 'group':\n",
    "                # Skip group if no parameters have gradients\n",
    "                valid_params = [p for p in group['params'] if p.grad is not None]\n",
    "                if not valid_params:\n",
    "                    continue\n",
    "                \n",
    "                all_exp_avg = []\n",
    "                all_exp_avg_sq = []\n",
    "                \n",
    "                # First pass: update momentum and collect states\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    \n",
    "                    self._validate_gradient(p.grad, group_idx)\n",
    "                    state = self._init_param_state(p)\n",
    "                    self._update_moments(state, p.grad, beta1, beta2)\n",
    "                    \n",
    "                    all_exp_avg.append(state['exp_avg'].flatten())\n",
    "                    all_exp_avg_sq.append(state['exp_avg_sq'].flatten())\n",
    "                \n",
    "                # Compute group-level AGAR using pre-allocated buffers\n",
    "                if all_exp_avg:\n",
    "                    # Allocate buffers on first use (amortized across all steps)\n",
    "                    if group_state['reuse_buffer_exp_avg'] is None:\n",
    "                        total_params = sum(m.numel() for m in all_exp_avg)\n",
    "                        device = all_exp_avg[0].device\n",
    "                        dtype = all_exp_avg[0].dtype\n",
    "                        group_state['reuse_buffer_exp_avg'] = torch.zeros(total_params, device=device, dtype=dtype)\n",
    "                        group_state['reuse_buffer_exp_avg_sq'] = torch.zeros(total_params, device=device, dtype=dtype)\n",
    "                    \n",
    "                    # Copy moment estimates into buffers (avoids repeated allocations)\n",
    "                    offset = 0\n",
    "                    reuse_buffer_exp_avg = group_state['reuse_buffer_exp_avg']\n",
    "                    reuse_buffer_exp_avg_sq = group_state['reuse_buffer_exp_avg_sq']\n",
    "                    \n",
    "                    for m, v in zip(all_exp_avg, all_exp_avg_sq):\n",
    "                        numel = m.numel()\n",
    "                        reuse_buffer_exp_avg[offset:offset+numel].copy_(m)\n",
    "                        reuse_buffer_exp_avg_sq[offset:offset+numel].copy_(v)\n",
    "                        offset += numel\n",
    "                    \n",
    "                    # Compute AGAR on concatenated moments\n",
    "                    agar = self._compute_agar(\n",
    "                        reuse_buffer_exp_avg[:offset],\n",
    "                        reuse_buffer_exp_avg_sq[:offset],\n",
    "                        eps,\n",
    "                        agar_clamp_factor\n",
    "                    )\n",
    "                    \n",
    "                    agar_value = agar.item()\n",
    "                    group_state['current_agar'] = agar_value\n",
    "                    \n",
    "                    # Tau calibration and adaptation\n",
    "                    if not group_state['tau_initialized']:\n",
    "                        group_state['agar_buffer'].append(agar_value)\n",
    "                        \n",
    "                        # Diagnostic logging during calibration\n",
    "                        if self._step_count % 10 == 0 and len(group_state['agar_buffer']) > 0:\n",
    "                            agars = list(group_state['agar_buffer'])\n",
    "                            self._log(3, f\"Step {self._step_count} - AGAR: min={min(agars):.4f}, median={np.median(agars):.4f}, max={max(agars):.4f}\")\n",
    "                        \n",
    "                        if len(group_state['agar_buffer']) >= tau_init_steps:\n",
    "                            tau = self._calibrate_tau(group_state['agar_buffer'], tau_clip_range, group_idx)\n",
    "                            group_state['tau_adapter'].tau = tau\n",
    "                            group_state['tau_adapter'].tau_calibrated = tau  # Anchor dead zone to calibrated value\n",
    "                            group_state['tau_initialized'] = True\n",
    "                            group_state['agar_buffer'].clear()\n",
    "                            self._log(2, f\"Group {group_idx}: Tau calibrated to {tau:.4f} from {tau_init_steps} samples\")\n",
    "                    else:\n",
    "                        # Post-calibration: adapt tau using drift-detecting EMA\n",
    "                        group_state['tau_adapter'].update(agar_value)\n",
    "                    \n",
    "                    # Universal sigmoid-based confidence mapping\n",
    "                    if group_state['tau_initialized']:\n",
    "                        mu = group_state.get('agar_mean', agar_value)\n",
    "                        sigma = group_state.get('agar_std', 0.1)\n",
    "                        c_min_adaptive = group_state.get('c_min', c_min)\n",
    "                        \n",
    "                        # Sigmoid mapping: confidence = c_min + (1 - c_min) * sigmoid((agar - μ) / σ)\n",
    "                        # This naturally adapts to any distribution:\n",
    "                        # - High μ, low σ (clean): Most samples get high confidence\n",
    "                        # - Low μ, low σ (pervasive noise): Confidence scales smoothly from c_min\n",
    "                        # - High σ (mixed): Strong discrimination between low/high AGAR\n",
    "                        z_score = (agar_value - mu) / sigma\n",
    "                        sigmoid = 1.0 / (1.0 + np.exp(-z_score))\n",
    "                        confidence_value = c_min_adaptive + (1.0 - c_min_adaptive) * sigmoid\n",
    "                        \n",
    "                        confidence_value = float(np.clip(confidence_value, c_min_adaptive, 1.0))\n",
    "                    else:\n",
    "                        # Pre-calibration: simple passthrough\n",
    "                        confidence_value = float(np.clip(agar_value, c_min, 1.0))\n",
    "                    \n",
    "                    group_state['current_confidence'] = confidence_value\n",
    "                    \n",
    "                    # Diagnostic logging\n",
    "                    if group_state['tau_initialized'] and self._step_count % 100 == 0:\n",
    "                        mu = group_state.get('agar_mean', 0)\n",
    "                        sigma = group_state.get('agar_std', 0)\n",
    "                        self._log(3, f\"Step {self._step_count} - AGAR={agar_value:.4f}, μ={mu:.4f}, \"\n",
    "                                     f\"σ={sigma:.4f}, Confidence={confidence_value:.4f}\")\n",
    "                    \n",
    "                    confidence_tensor = torch.tensor(confidence_value, device=all_exp_avg[0].device, dtype=all_exp_avg[0].dtype)\n",
    "                else:\n",
    "                    confidence_tensor = torch.tensor(c_min)\n",
    "                \n",
    "                # Apply parameter updates with confidence-scaled learning rate\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    \n",
    "                    self._apply_weight_update(p, self.state[p], lr, weight_decay, \n",
    "                                             eps, confidence_tensor, beta1, beta2)\n",
    "            \n",
    "            # Per-parameter AGAR mode: compute separately for each parameter\n",
    "            else:\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    \n",
    "                    self._validate_gradient(p.grad, group_idx)\n",
    "                    state = self._init_param_state(p)\n",
    "                    self._update_moments(state, p.grad, beta1, beta2)\n",
    "                    \n",
    "                    # Compute per-parameter AGAR\n",
    "                    agar = self._compute_agar(state['exp_avg'], state['exp_avg_sq'], eps, agar_clamp_factor)\n",
    "                    \n",
    "                    agar_value = agar.item()\n",
    "                    \n",
    "                    # Store first parameter's AGAR as representative for monitoring\n",
    "                    if group_state['current_agar'] is None:\n",
    "                        group_state['current_agar'] = agar_value\n",
    "                    \n",
    "                    # Tau calibration and adaptation\n",
    "                    if not group_state['tau_initialized']:\n",
    "                        group_state['agar_buffer'].append(agar_value)\n",
    "                        \n",
    "                        # Diagnostic logging during calibration\n",
    "                        if self._step_count % 10 == 0 and len(group_state['agar_buffer']) > 0:\n",
    "                            agars = list(group_state['agar_buffer'])\n",
    "                            self._log(3, f\"Step {self._step_count} - AGAR: min={min(agars):.4f}, median={np.median(agars):.4f}, max={max(agars):.4f}\")\n",
    "                        \n",
    "                        if len(group_state['agar_buffer']) >= tau_init_steps:\n",
    "                            tau = self._calibrate_tau(group_state['agar_buffer'], tau_clip_range, group_idx)\n",
    "                            group_state['tau_adapter'].tau = tau\n",
    "                            group_state['tau_adapter'].tau_calibrated = tau  # Anchor dead zone to calibrated value\n",
    "                            group_state['tau_initialized'] = True\n",
    "                            group_state['agar_buffer'].clear()\n",
    "                            self._log(2, f\"Group {group_idx}: Tau calibrated to {tau:.4f} from {tau_init_steps} samples\")\n",
    "                    else:\n",
    "                        # Post-calibration: adapt tau using drift-detecting EMA\n",
    "                        group_state['tau_adapter'].update(agar_value)\n",
    "                    \n",
    "                    # Universal sigmoid-based confidence mapping\n",
    "                    if group_state['tau_initialized']:\n",
    "                        mu = group_state.get('agar_mean', agar_value)\n",
    "                        sigma = group_state.get('agar_std', 0.1)\n",
    "                        c_min_adaptive = group_state.get('c_min', c_min)\n",
    "                        \n",
    "                        # Sigmoid mapping: confidence = c_min + (1 - c_min) * sigmoid((agar - μ) / σ)\n",
    "                        z_score = (agar_value - mu) / sigma\n",
    "                        sigmoid = 1.0 / (1.0 + np.exp(-z_score))\n",
    "                        confidence_value = c_min_adaptive + (1.0 - c_min_adaptive) * sigmoid\n",
    "                        \n",
    "                        confidence_value = float(np.clip(confidence_value, c_min_adaptive, 1.0))\n",
    "                    else:\n",
    "                        # Pre-calibration: simple passthrough\n",
    "                        confidence_value = float(np.clip(agar_value, c_min, 1.0))\n",
    "                    \n",
    "                    confidence_tensor = torch.tensor(confidence_value, device=p.device, dtype=p.dtype)\n",
    "                    \n",
    "                    # Store first parameter's confidence as representative for monitoring\n",
    "                    if group_state['current_confidence'] is None:\n",
    "                        group_state['current_confidence'] = confidence_value\n",
    "                    \n",
    "                    # Diagnostic logging\n",
    "                    if group_state['tau_initialized'] and self._step_count % 100 == 0:\n",
    "                        mu = group_state.get('agar_mean', 0)\n",
    "                        sigma = group_state.get('agar_std', 0)\n",
    "                        self._log(3, f\"Step {self._step_count} - AGAR={agar_value:.4f}, μ={mu:.4f}, \"\n",
    "                                     f\"σ={sigma:.4f}, Confidence={confidence_value:.4f}\")\n",
    "                    \n",
    "                    # Apply parameter update\n",
    "                    self._apply_weight_update(p, state, lr, weight_decay, \n",
    "                                             eps, confidence_tensor, beta1, beta2)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def state_dict(self):\n",
    "        \"\"\"\n",
    "        Return the optimizer state as a dictionary.\n",
    "        \n",
    "        Includes all parameter states, hyperparameters, and internal calibration data.\n",
    "        Compatible with torch.save() for checkpointing.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Complete optimizer state including:\n",
    "                - Parameter-specific states (exp_avg, exp_avg_sq, step)\n",
    "                - Group-level calibration data (tau, agar_buffer)\n",
    "                - Global step counter\n",
    "        \"\"\"\n",
    "        state_dict = super().state_dict()\n",
    "        \n",
    "        # Serialize group states (convert deque to list)\n",
    "        serializable_group_states = {}\n",
    "        for idx, gs in self._group_states.items():\n",
    "            serializable_group_states[idx] = {\n",
    "                'tau_initialized': gs['tau_initialized'],\n",
    "                'agar_buffer': list(gs['agar_buffer']),\n",
    "                'agar_buffer_maxlen': gs['agar_buffer'].maxlen,\n",
    "                'adapter_tau': gs['tau_adapter'].tau,\n",
    "                'adapter_tau_calibrated': gs['tau_adapter'].tau_calibrated,\n",
    "                'adapter_mean': gs['tau_adapter'].mean_agar,\n",
    "                'adapter_var': gs['tau_adapter'].ema_var,\n",
    "                'agar_mean': gs.get('agar_mean'),\n",
    "                'agar_std': gs.get('agar_std'),\n",
    "                'agar_median': gs.get('agar_median'),\n",
    "                'agar_p10': gs.get('agar_p10'),\n",
    "                'agar_p90': gs.get('agar_p90'),\n",
    "                'c_min': gs.get('c_min'),\n",
    "            }\n",
    "        \n",
    "        state_dict['_group_states'] = serializable_group_states\n",
    "        state_dict['_step_count'] = self._step_count\n",
    "        \n",
    "        return state_dict\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"\n",
    "        Load optimizer state from a dictionary.\n",
    "        \n",
    "        Restores all parameter states, hyperparameters, and internal calibration data.\n",
    "        Compatible with torch.load() for checkpoint restoration.\n",
    "        \n",
    "        Args:\n",
    "            state_dict (dict): Optimizer state dictionary (typically from state_dict())\n",
    "        \n",
    "        Note:\n",
    "            Automatically handles conversion of serialized lists back to deque objects\n",
    "            for AGAR buffer management.\n",
    "        \"\"\"\n",
    "        # Restore group states (convert list back to deque)\n",
    "        if '_group_states' in state_dict:\n",
    "            loaded_states = state_dict.pop('_group_states')\n",
    "            # Ensure keys are integers (they may be strings after JSON serialization)\n",
    "            self._group_states = {}\n",
    "            for idx, gs in loaded_states.items():\n",
    "                idx_int = int(idx) if isinstance(idx, str) else idx\n",
    "                \n",
    "                maxlen = gs.pop('agar_buffer_maxlen', None)\n",
    "                buffer_list = gs.pop('agar_buffer', [])\n",
    "                gs['agar_buffer'] = deque(buffer_list, maxlen=maxlen)\n",
    "                \n",
    "                # Restore adapter state\n",
    "                tau_clip_range = self.param_groups[idx_int]['tau_clip_range']\n",
    "                tau_dead_zone = self.param_groups[idx_int]['tau_dead_zone']\n",
    "                adapter = DDEAdapter(1.0, tau_clip_range, dead_zone_factor=tau_dead_zone)\n",
    "                adapter.tau = gs.pop('adapter_tau', 1.0)\n",
    "                adapter.tau_calibrated = gs.pop('adapter_tau_calibrated', None)\n",
    "                adapter.mean_agar = gs.pop('adapter_mean', 1.0)\n",
    "                adapter.ema_var = gs.pop('adapter_var', 0.01)\n",
    "                gs['tau_adapter'] = adapter\n",
    "                \n",
    "                # Initialize missing fields\n",
    "                gs.setdefault('reuse_buffer_exp_avg', None)\n",
    "                gs.setdefault('reuse_buffer_exp_avg_sq', None)\n",
    "                gs.setdefault('current_agar', None)\n",
    "                gs.setdefault('current_confidence', None)\n",
    "                gs.setdefault('agar_mean', None)\n",
    "                gs.setdefault('agar_std', None)\n",
    "                gs.setdefault('agar_median', None)\n",
    "                gs.setdefault('agar_p10', None)\n",
    "                gs.setdefault('agar_p90', None)\n",
    "                gs.setdefault('c_min', self.param_groups[idx_int].get('c_min', 0.1))\n",
    "                \n",
    "                self._group_states[idx_int] = gs\n",
    "        \n",
    "        # Restore step count\n",
    "        if '_step_count' in state_dict:\n",
    "            self._step_count = state_dict.pop('_step_count')\n",
    "        \n",
    "        super().load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03caf23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: CASMO | Noise: 30.0%\n",
      "Batch: 8 | Accum: 4 | Effective Batch: 32\n",
      "============================================================\n",
      "\n",
      "Loading microsoft/phi-2 with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99333c873ee4a1cb0549ae23ead283b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c5119d106e4e3597ea30232d01f537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff103b40e1b4aada742a91bcfc7586d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a18afcd8544233931d4ccf8d589611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ccb7909dc84ff885e863aaa1f8da5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00090d99a45f479da2ccd3abf93815e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cfa6867d2a4ce7b5c63a3fbcb7683b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f40a197432422daf66074b8c253b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adbdafd1ecb4198a5414f10a1d4c19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3c756c29114162a2fc7bffccca5134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0e8c56d5ed491082dad3cb21da6bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f39cf92b0a04440abb33e063e4f2212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63de7dcc2be247f6aa589825d95bdde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,107,200 || all params: 2,792,791,040 || trainable%: 0.4693\n",
      "Loading dataset: HuggingFaceH4/ultrafeedback_binarized [train]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11157717c8d34e12959d2ef22dab7fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa5613ab2a544e9bdb27da200842a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_prefs-00000-of-00001.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147cc9b42a174c8b953538a2e9fdac6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test_prefs-00000-of-00001.parquet:   0%|          | 0.00/7.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8332cd1b3a43e28a297735d7748080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test_sft-00000-of-00001.parquet:   0%|          | 0.00/3.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb546c1f4624cf08fd6dc725748f7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_gen-00000-of-00001.parquet:   0%|          | 0.00/184M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb12e2c863d4747976e2353163e9ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test_gen-00000-of-00001.parquet:   0%|          | 0.00/3.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5e897bd22745b4b8ff98b4498a40c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_prefs split:   0%|          | 0/61135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397b7a80cc8642018190cc6885dcda34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_sft split:   0%|          | 0/61135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92bd9f5ab2c440da68194ea18bdc59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_prefs split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fa961aeeb14f668947707938d431ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_sft split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab00688ca4954e23bc5a66bbdf5666c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_gen split:   0%|          | 0/61135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd31816dc6546c6bc461ef26d3215e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_gen split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10000 samples with 30.0% noise...\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1250 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Training:  46%|████▋     | 580/1250 [1:20:26<1:35:48,  8.58s/it, loss=1.2656]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Benchmark B8: Noisy Instruction Following (Phi-2 + QLoRA)\n",
    "\n",
    "Tests CASMO's ability to filter out noisy instruction data (misaligned preferences) \n",
    "during LLM fine-tuning.\n",
    "\n",
    "Model: Microsoft Phi-2 (2.7B)\n",
    "Technique: QLoRA (4-bit)\n",
    "Dataset: UltraFeedback (30% Noise Injected)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Setup\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_model_and_tokenizer(model_name=\"microsoft/phi-2\"):\n",
    "    print(f\"Loading {model_name} with 4-bit quantization...\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # LoRA Config\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"Wqkv\", \"out_proj\", \"fc1\", \"fc2\"], # Phi-2 specific modules\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Training\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def run_training(optimizer_name, epochs=1, batch_size=8, grad_accum_steps=4, lr=2e-4, noise_ratio=0.3):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {optimizer_name.upper()} | Noise: {noise_ratio*100}%\")\n",
    "    print(f\"Batch: {batch_size} | Accum: {grad_accum_steps} | Effective Batch: {batch_size*grad_accum_steps}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    model, tokenizer = get_model_and_tokenizer()\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset = NoisyInstructDataset(tokenizer, split='train', noise_ratio=noise_ratio)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Optimizer\n",
    "    if optimizer_name == 'casmo':\n",
    "        optimizer = CASMO(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            tau_init_steps=100, # Fast calibration for short benchmark\n",
    "            weight_decay=0.01,\n",
    "            granularity='group', # 'group' is recommended for large models\n",
    "            betas=(0.9, 0.999),\n",
    "            log_level=1\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "    results = {\n",
    "        'losses': [],\n",
    "        'noisy_confidences': [],\n",
    "        'clean_confidences': [],\n",
    "        'steps': []\n",
    "    }\n",
    "    \n",
    "    step = 0\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        current_batch_noisy = []\n",
    "        \n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "            labels = batch['labels'].to(model.device)\n",
    "            is_noisy = batch['is_noisy'] # [Batch]\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / grad_accum_steps # Scale loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Track noise for the accumulated batch\n",
    "            current_batch_noisy.append(is_noisy.float().mean().item())\n",
    "            \n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Log Metrics (once per effective step)\n",
    "                current_loss = loss.item() * grad_accum_steps\n",
    "                results['losses'].append(current_loss)\n",
    "                results['steps'].append(global_step)\n",
    "                \n",
    "                # CASMO Confidence Tracking\n",
    "                if optimizer_name == 'casmo':\n",
    "                    # Get average confidence across all groups\n",
    "                    confs = []\n",
    "                    for g_id, state in optimizer._group_states.items():\n",
    "                        if 'current_confidence' in state:\n",
    "                            confs.append(state['current_confidence'])\n",
    "                    \n",
    "                    avg_conf = np.mean(confs) if confs else 1.0\n",
    "                    \n",
    "                    # Heuristic: If >50% of the *accumulated* batch is noisy\n",
    "                    avg_noise_ratio = np.mean(current_batch_noisy)\n",
    "                    if avg_noise_ratio > 0.5:\n",
    "                        results['noisy_confidences'].append(avg_conf)\n",
    "                    else:\n",
    "                        results['clean_confidences'].append(avg_conf)\n",
    "                \n",
    "                current_batch_noisy = [] # Reset\n",
    "                progress_bar.set_postfix({'loss': f\"{current_loss:.4f}\"})\n",
    "                global_step += 1\n",
    "                \n",
    "                if global_step >= 150: # Run enough steps to see post-calibration behavior\n",
    "                    print(\"Step limit reached (150). Stopping.\")\n",
    "                    break\n",
    "        \n",
    "        if global_step >= 150:\n",
    "            break\n",
    "            \n",
    "    return results\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():    \n",
    "    # Run CASMO\n",
    "    casmo_res = run_training('casmo', epochs=1, batch_size=8, noise_ratio=0.3)\n",
    "    \n",
    "    # Run AdamW\n",
    "    adam_res = run_training('adamw', epochs=1, batch_size=8, noise_ratio=0.3)\n",
    "    \n",
    "    # Plot\n",
    "    results_dir = 'results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(casmo_res['losses'], label='CASMO', alpha=0.7)\n",
    "    plt.plot(adam_res['losses'], label='AdamW', alpha=0.7)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Confidence\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if casmo_res['noisy_confidences']:\n",
    "        plt.hist(casmo_res['clean_confidences'], alpha=0.5, label='Clean Batches', bins=20, color='green')\n",
    "        plt.hist(casmo_res['noisy_confidences'], alpha=0.5, label='Noisy Batches', bins=20, color='red')\n",
    "        plt.title('CASMO Confidence Distribution')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'instruct_results.png'))\n",
    "    print(f\"Results saved to {results_dir}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
